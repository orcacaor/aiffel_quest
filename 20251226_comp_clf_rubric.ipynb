{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOL1slpR48IggH6Nbydpjpr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ì•„ì§ ì‘ì„±ì¤‘ì…ë‹ˆë‹¤ã…¡ã…¡ã…¡ã…¡ã…¡ì‹¤í–‰í•´ì„œ ê²°ê³¼ë„ ë‚˜ì˜¤ë„ë¡ í•  ì˜ˆì •ì…ë‹ˆë‹¤ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡# (ì™„ë£Œì‹œ í•´ë‹¹ ì£¼ì„ ì‚¬ë¼ì§))"
      ],
      "metadata": {
        "id": "O6VRh8R1W21l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "YtT6nc3aW7Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "fgVQDk0AW7hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OYu7peI02-BM"
      },
      "outputs": [],
      "source": [
        "# ë§ˆê°ì‹œê°„ì´ ë‹¤ ë¼ì„œ ìš°ì„  í‰ê°€ìš© ì½”ë“œë¥¼ ì œì¶œí•©ë‹ˆë‹¤\n",
        "# ì½”ë“œê°€ ì½”ë©, vsì½”ë“œì— ë„ˆë¬´ ë§ì´ í©ì–´ì ¸ìˆì–´ì„œ ì£¼ì„¬ì£¼ì„¬ ëª¨ì•˜ìŠµë‹ˆë‹¤\n",
        "\n",
        "# ì½”ë“œ ë° ì£¼ì„ì€ LLM ì—ê²Œ ì§€ì‹œí•´ì„œ ìƒì„±ëœ ê²ƒì´ë‹ˆ ì½ìœ¼ì‹¤ í•„ìš”ëŠ” ì—†ê³ \n",
        "# ì œ ì˜ë„ ë° ì„¤ëª…ì„ #ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ << ì´ë ‡ê²Œ ìƒê¸´ ì£¼ì„ì— ë‹¬ì•„ë‘ì—ˆìŠµë‹ˆë‹¤\n",
        "\n",
        "#ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ ì˜ë„ì˜ë„ì˜ë„"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì˜¤ë²„ìƒ˜í”Œë§ SMOTE\n",
        "# ì–¸ë”ìƒ˜í”Œë§\n",
        "# ì„œë¸Œì…‹\n",
        "\n",
        "# ì´ì§€ì—”ì…ˆë¸” - ë¡œì§€ìŠ¤í‹±, LGBM, XGB, CAT, SVM, ì—ì´ë‹¤ë¶€ìŠ¤íŠ¸\n",
        "# ë°¸ëŸ°ìŠ¤ë“œ ìºìŠ¤ì¼€ì´ë“œ -\n",
        "# ë°¸ëŸ°ìŠ¤ë“œ ëœë¤í¬ë ˆìŠ¤íŠ¸\n",
        "\n",
        "# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
        "# Ví”¼ì²˜ë¼ë¦¬ ìƒí˜¸ì‘ìš©\n",
        "# Ví”¼ì²˜ Amount ìƒí˜¸ì‘ìš©\n",
        "# Amount ë¡œê·¸, ë¡œë²„ìŠ¤íŠ¸ìŠ¤ì¼€ì¼ë§\n",
        "# Time ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§, hour ë³€í™˜, hour ì‚¬ì¸, hour ì½”ì‚¬ì¸, ë‚® ì‹œê°„ëŒ€\n",
        "\n",
        "# autoencoder (ë”¥ëŸ¬ë‹)\n",
        "# isolation forest\n",
        "\n",
        "# LGBM XGB CAT ë¸”ë Œë”©\n",
        "# LGBM XGB CAT ìŠ¤íƒœí‚¹\n",
        "\n",
        "# í¼ë¸”ë¦­ ë¦¬ë”ë³´ë“œ í•´í‚¹ ì„ê³„ê°’ ì¡°ì ˆ\n",
        "\n",
        "# ë˜ ë­í–ˆì§€? ë„ˆë¬´ ë§ì´ í•´ì„œ ì „ë¶€ ê¸°ì–µë„ ì•ˆë‚¨.\n",
        "# focal loss\n"
      ],
      "metadata": {
        "id": "MzYjpGQD4fa6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ìš”êµ¬ì‚¬í•­\n",
        "\n",
        "# - ë°ì´í„° ì´í•´ ë° íƒìƒ‰: Classì˜ ê· í˜•ê³¼ ë°ì´í„°ì˜ ë¶„í¬ì™€ íŠ¹ì„±ì„ ì‹œê°í™”ë¥¼ í†µí•´ì„œ íŒŒì•…í–ˆëŠ”ê°€?\n",
        "# - ëª¨ë¸ ì„ íƒ ë° ë¹„êµ: 3ê°œ ì´ìƒì˜ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³  ì„±ëŠ¥ì„ ë¹„êµí–ˆëŠ”ê°€?\n",
        "# - ë ˆì´ë¸” ë¶ˆê· í˜•ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ê¸°ë²•ì„ ì‹œë„í–ˆëŠ”ê°€?\n",
        "# - ì—¬ëŸ¬ ëª¨ë¸ì„ ì•™ìƒë¸”ì„ í†µí•´ ê²°í•©í•˜ì—¬ ì„±ëŠ¥ì„ ë¹„êµí–ˆëŠ”ê°€?\n",
        "# - Ridge/Lasso ë“±ì˜ ê·œì œ ì¶”ê°€, íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ì˜ ê¹Šì´/í•™ìŠµë¥  ë“± í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒì„ ì‹œë„í–ˆëŠ”ê°€?"
      ],
      "metadata": {
        "id": "hlP6vpVo3nQ6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testestesteststest\n",
        "#gm ì˜¬ë¦¬ê³  (1)\n",
        "# ì´ì§€ì—”ì…ˆë¸” ì—¬ëŸ¬ê°€ì§€ ì˜¬ë¦¬ê³  - 1\n",
        "# ë°¸ëŸ°ìŠ¤ë“œëœë¤í¬ë ˆìŠ¤íŠ¸?\n",
        "# ì„œë¸Œë¯¸ì…˜21? ê·¸ê±° ì˜¬ë¦¬ê³  - 2\n",
        "# ë§ˆì§€ë§‰ì— ë¸”ë Œë”©í•œê±° ì˜¬ë¦¬ì - 3"
      ],
      "metadata": {
        "id": "W8MMq_di3nOD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Cc1MWSAE3nIz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "submission = pd.read_csv('sample_submission.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "U5HA7Y6P3nEu",
        "outputId": "e8226990-9a1c-449d-da41-9f1ac61060d5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2880055081.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['Class'].value_counts()"
      ],
      "metadata": {
        "id": "LfcfFSdg3nCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# ë°ì´í„° ë¡œë“œ\n",
        "# train = pd.read_csv('train.csv')\n",
        "# test = pd.read_csv('test.csv')\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. ID ì»¬ëŸ¼ ì‚­ì œ\n",
        "# ---------------------------------------------------------\n",
        "if 'id' in train.columns:\n",
        "    train = train.drop('id', axis=1)\n",
        "if 'id' in test.columns:\n",
        "    test = test.drop('id', axis=1)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Time, Amount ìŠ¤ì¼€ì¼ë§ -> ìƒˆë¡œìš´ ì»¬ëŸ¼ ìƒì„±\n",
        "# ---------------------------------------------------------\n",
        "rs = RobustScaler()\n",
        "cols = ['Time', 'Amount']\n",
        "\n",
        "# ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ë° ë³€í™˜ (Trainì€ fit_transform, TestëŠ” transform)\n",
        "# ê²°ê³¼ëŠ” numpy array í˜•íƒœ([í–‰, 2])ë¡œ ë°˜í™˜ë©ë‹ˆë‹¤. (0ë²ˆ ì¸ë±ìŠ¤: Time, 1ë²ˆ ì¸ë±ìŠ¤: Amount)\n",
        "train_scaled = rs.fit_transform(train[cols])\n",
        "test_scaled = rs.transform(test[cols])\n",
        "\n",
        "# ìƒˆë¡œìš´ ì»¬ëŸ¼ì— í• ë‹¹\n",
        "train['scaled_time'] = train_scaled[:, 0]\n",
        "train['scaled_amount'] = train_scaled[:, 1]\n",
        "\n",
        "test['scaled_time'] = test_scaled[:, 0]\n",
        "test['scaled_amount'] = test_scaled[:, 1]\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. ì›ë³¸ Time, Amount ì»¬ëŸ¼ ì‚­ì œ\n",
        "# ---------------------------------------------------------\n",
        "train = train.drop(cols, axis=1)\n",
        "test = test.drop(cols, axis=1)\n",
        "\n",
        "# ê²°ê³¼ í™•ì¸\n",
        "print(\"ë³€ê²½ëœ Train ì»¬ëŸ¼ ëª©ë¡:\", train.columns.tolist())\n",
        "display(train.head())"
      ],
      "metadata": {
        "id": "WoQEL9733nAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()"
      ],
      "metadata": {
        "id": "0gBIjDu23m99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Classë³„ë¡œ ë°ì´í„°ë¥¼ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
        "fraud_df = train[train['Class'] == 1]\n",
        "normal_df = train[train['Class'] == 0]\n",
        "\n",
        "print(f\"ì‚¬ê¸° ê±°ë˜ ê°œìˆ˜: {len(fraud_df)}\")\n",
        "print(f\"ì •ìƒ ê±°ë˜ ê°œìˆ˜: {len(normal_df)}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Classê°€ 0ì¸ ê²ƒì„ Classê°€ 1ì¸ ê°œìˆ˜ë§Œí¼ ë¬´ì‘ìœ„ ìƒ˜í”Œë§\n",
        "# ---------------------------------------------------------\n",
        "# random_state=42ëŠ” ì‹¤í–‰í•  ë•Œë§ˆë‹¤ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì˜¤ë„ë¡ ê³ ì •í•˜ëŠ” ì‹œë“œê°’ì…ë‹ˆë‹¤.\n",
        "normal_sampled = normal_df.sample(n=len(fraud_df), random_state=42)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. ë°ì´í„° í•©ì¹˜ê¸° ë° ì„ê¸° (Shuffle)\n",
        "# ---------------------------------------------------------\n",
        "# ë‘ ë°ì´í„°í”„ë ˆì„ì„ í•©ì¹©ë‹ˆë‹¤.\n",
        "sub_train = pd.concat([fraud_df, normal_sampled], axis=0)\n",
        "\n",
        "# ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ìŠµë‹ˆë‹¤.\n",
        "# frac=1ì€ ì „ì²´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ë˜ ìˆœì„œë¥¼ ì„ëŠ”ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\n",
        "sub_train = sub_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# ê²°ê³¼ í™•ì¸\n",
        "print(\"-\" * 30)\n",
        "print(\"ì„œë¸Œ ë°ì´í„°ì…‹(sub_train) í¬ê¸°:\", sub_train.shape)\n",
        "print(\"ì„œë¸Œ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ë¶„í¬:\\n\", sub_train['Class'].value_counts())\n",
        "display(sub_train.head())"
      ],
      "metadata": {
        "id": "Tx0tTwD-3m7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°\n",
        "# ---------------------------------------------------------\n",
        "corr_original = train.corr()\n",
        "corr_sub = sub_train.corr()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. íˆíŠ¸ë§µ ì‹œê°í™” í•¨ìˆ˜ ì •ì˜\n",
        "# ---------------------------------------------------------\n",
        "def plot_heatmap(corr_matrix, title):\n",
        "    # ìº”ë²„ìŠ¤ í¬ê¸° ì„¤ì •\n",
        "    plt.figure(figsize=(24, 20))\n",
        "\n",
        "    # íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°\n",
        "    # cmap='coolwarm_r': íŒŒë€ìƒ‰(ìŒ) ~ ë¹¨ê°„ìƒ‰(ì–‘) ( _rì€ reverse)\n",
        "    # vmin=-1, vmax=1: ìƒê´€ê´€ê³„ ë²”ìœ„ ê³ ì •\n",
        "    # annot=True: ì¹¸ ì•ˆì— ìˆ˜ì¹˜ í‘œì‹œ\n",
        "    # fmt='.2f': ì†Œìˆ˜ì  2ìë¦¬ê¹Œì§€ í‘œì‹œ\n",
        "    sns.heatmap(corr_matrix, cmap='coolwarm_r', annot=True, fmt='.2f',\n",
        "                linewidths=0.2, vmin=-1, vmax=1)\n",
        "\n",
        "    plt.title(title, fontsize=20)\n",
        "    plt.show()\n",
        "\n",
        "# 1) ì›ë³¸ ë°ì´í„°ì…‹ íˆíŠ¸ë§µ\n",
        "print(\"### 1. ì›ë³¸ ë°ì´í„°ì…‹(ë¶ˆê· í˜•) ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ###\")\n",
        "plot_heatmap(corr_original, \"Correlation Matrix (Original Imbalanced)\")\n",
        "\n",
        "# 2) ì„œë¸Œì…‹ íˆíŠ¸ë§µ\n",
        "print(\"\\n### 2. ì„œë¸Œ ë°ì´í„°ì…‹(ê· í˜•) ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ###\")\n",
        "# ì´ íˆíŠ¸ë§µì´ ì‚¬ê¸° íƒì§€ì— ì‹¤ì§ˆì ìœ¼ë¡œ ì¤‘ìš”í•œ í”¼ì²˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
        "plot_heatmap(corr_sub, \"Correlation Matrix (SubSampled Balanced)\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. ìƒê´€ê´€ê³„ ìˆ˜ì¹˜ ì •ë ¬ ë° ìƒìœ„/í•˜ìœ„ í”¼ì²˜ í™•ì¸\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n### 3. 'Class'ì™€ì˜ ìƒê´€ê´€ê³„ ë¶„ì„ (ì„œë¸Œì…‹ ê¸°ì¤€) ###\")\n",
        "\n",
        "# Class ì»¬ëŸ¼ê³¼ì˜ ìƒê´€ê³„ìˆ˜ë§Œ ì¶”ì¶œ (Class ìì‹ ì€ 1ì´ë¯€ë¡œ ì œì™¸)\n",
        "corr_with_class = corr_sub['Class'].drop('Class')\n",
        "\n",
        "# ì •ë ¬\n",
        "sorted_corr = corr_with_class.sort_values(ascending=False)\n",
        "\n",
        "# ì‹œê°í™” (ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ í•œëˆˆì— ë³´ê¸°)\n",
        "plt.figure(figsize=(12, 8))\n",
        "colors = ['red' if x > 0 else 'blue' for x in sorted_corr.values]\n",
        "sorted_corr.plot(kind='bar', color=colors)\n",
        "plt.title('Feature Correlations with Class (SubSampled)', fontsize=15)\n",
        "plt.axhline(0, color='black', linewidth=1) # 0 ê¸°ì¤€ì„ \n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# í…ìŠ¤íŠ¸ë¡œ ìƒìœ„/í•˜ìœ„ ì¶œë ¥\n",
        "print(\"\\n[ì–‘ì˜ ìƒê´€ê´€ê³„ Top 5] (Classê°€ 1ì¼ ë•Œ ê°’ì´ ë†’ì•„ì§€ëŠ” ê²½í–¥)\")\n",
        "print(sorted_corr.head(5))\n",
        "\n",
        "print(\"\\n[ìŒì˜ ìƒê´€ê´€ê³„ Top 5] (Classê°€ 1ì¼ ë•Œ ê°’ì´ ë‚®ì•„ì§€ëŠ” ê²½í–¥)\")\n",
        "print(sorted_corr.tail(5))"
      ],
      "metadata": {
        "id": "Z8nEIBv63m04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. ì„œë¸Œ ë°ì´í„°ì…‹(sub_train) ê¸°ì¤€ ìƒê´€ê´€ê³„ ê³„ì‚°\n",
        "corr_matrix = sub_train.corr()\n",
        "\n",
        "# 2. 'Class' ì»¬ëŸ¼ê³¼ì˜ ìƒê´€ê³„ìˆ˜ë§Œ ì¶”ì¶œ (Class ìì‹ ì€ ì œì™¸)\n",
        "corr_with_class = corr_matrix['Class'].drop('Class')\n",
        "\n",
        "# 3. ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
        "top10_df = pd.DataFrame(corr_with_class)\n",
        "top10_df.columns = ['Correlation'] # ì»¬ëŸ¼ëª… ì„¤ì •\n",
        "\n",
        "# 4. ì ˆëŒ“ê°’(Abs) ì»¬ëŸ¼ ì¶”ê°€\n",
        "top10_df['Abs_Correlation'] = top10_df['Correlation'].abs()\n",
        "\n",
        "# 5. ì ˆëŒ“ê°’ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ Top 10 ì¶”ì¶œ\n",
        "top10_df = top10_df.sort_values(by='Abs_Correlation', ascending=False).head(10)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# ê²°ê³¼ ì¶œë ¥\n",
        "# ---------------------------------------------------------\n",
        "print(\"### 'Class'ì™€ì˜ ìƒê´€ê´€ê³„ ì ˆëŒ“ê°’ Top 10 ###\")\n",
        "display(top10_df)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# ì‹œê°í™” (ë°” ì°¨íŠ¸)\n",
        "# ---------------------------------------------------------\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# ìƒ‰ìƒ: ì–‘ì˜ ìƒê´€ê´€ê³„ëŠ” ë¹¨ê°•, ìŒì˜ ìƒê´€ê´€ê³„ëŠ” íŒŒë‘\n",
        "colors = ['red' if x > 0 else 'blue' for x in top10_df['Correlation']]\n",
        "\n",
        "sns.barplot(x=top10_df['Abs_Correlation'], y=top10_df.index, palette=colors)\n",
        "\n",
        "plt.title('Top 10 Features by Absolute Correlation with Class', fontsize=15)\n",
        "plt.xlabel('Absolute Correlation Coefficient')\n",
        "plt.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5) # 0.5 ê¸°ì¤€ì„ \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WuAh5CDELMaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. ìƒê´€ê³„ìˆ˜ ì ˆëŒ“ê°’ ê¸°ì¤€ ìƒìœ„ 5ê°œ í”¼ì²˜ ìë™ ì¶”ì¶œ\n",
        "# ---------------------------------------------------------\n",
        "# sub_trainì˜ ìƒê´€í–‰ë ¬ ê³„ì‚°\n",
        "corr_matrix = sub_train.corr()\n",
        "\n",
        "# Classì™€ì˜ ìƒê´€ê³„ìˆ˜ ì ˆëŒ“ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ ìƒìœ„ 5ê°œ ì´ë¦„ë§Œ ë½‘ê¸°\n",
        "top5_cols = corr_matrix['Class'].drop('Class').abs().sort_values(ascending=False).head(5).index.tolist()\n",
        "\n",
        "print(f\"âœ… ë¶„ì„ëœ Top 5 í”¼ì²˜ ëª©ë¡: {top5_cols}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. ìº”ë²„ìŠ¤ ì„¤ì • (1í–‰ 5ì—´ë¡œ ë³€ê²½)\n",
        "# ---------------------------------------------------------\n",
        "# ê·¸ë˜í”„ê°€ 5ê°œì´ë¯€ë¡œ ê°€ë¡œ ì‚¬ì´ì¦ˆë¥¼ 24ì—ì„œ 28ë¡œ ì¡°ê¸ˆ ëŠ˜ë ¸ìŠµë‹ˆë‹¤.\n",
        "f, axes = plt.subplots(ncols=5, figsize=(28, 6))\n",
        "\n",
        "# ìƒ‰ìƒ ì„¤ì • (íŒŒë€ìƒ‰: ì •ìƒ, ë¹¨ê°„ìƒ‰: ì‚¬ê¸°)\n",
        "colors = [\"#0101DF\", \"#DF0101\"]\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. ë°˜ë³µë¬¸ì„ ëŒë©° ê·¸ë˜í”„ ê·¸ë¦¬ê¸°\n",
        "# ---------------------------------------------------------\n",
        "for i, col in enumerate(top5_cols):\n",
        "    sns.boxplot(x=\"Class\", y=col, data=sub_train, palette=colors, ax=axes[i])\n",
        "\n",
        "    # ê·¸ë˜í”„ ì œëª© ë° ìŠ¤íƒ€ì¼ ì„¤ì •\n",
        "    axes[i].set_title(f'{col} vs Class', fontsize=16)\n",
        "    axes[i].set_xlabel('') # xì¶• ë¼ë²¨(Class) ì¤‘ë³µ ì œê±°\n",
        "    axes[i].grid(axis='y', linestyle='--', alpha=0.5) # ê²©ìì„  ì¶”ê°€\n",
        "\n",
        "plt.suptitle('Boxplots of Top 5 Correlated Features', fontsize=20, y=1.05)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rNjjy3TXLONj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. ì„¤ì •\n",
        "# ---------------------------------------------------------\n",
        "target_features = ['V14', 'V11', 'V10']  # ì²˜ë¦¬í•  í”¼ì²˜\n",
        "max_loss_limit = 0.03             # í—ˆìš©í•  ìµœëŒ€ ì •ë³´ ì†ì‹¤ë¥  (3%)\n",
        "# -> ì‚¬ê¸° ë°ì´í„° 100ê°œ ì¤‘ 3ê°œê¹Œì§€ë§Œ ì‚­ì œë¥¼ í—ˆìš©. ê·¸ ì´ìƒì€ ìœ„í—˜í•˜ë‹¤ê³  íŒë‹¨.\n",
        "\n",
        "total_fraud_count = len(sub_train[sub_train['Class'] == 1])\n",
        "best_factor = 1.5  # ì´ˆê¸°ê°’ (ëª» ì°¾ìœ¼ë©´ ê¸°ë³¸ 1.5 ì‚¬ìš©)\n",
        "\n",
        "print(f\"[{'Factor':^6}] | {'ì‚­ì œ ì˜ˆìƒ':^8} | {'ì†ì‹¤ë¥ (%)':^10} | {'íŒë‹¨'}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. ìµœì ì˜ Factor ì°¾ê¸° (ì‹œë®¬ë ˆì´ì…˜)\n",
        "# ---------------------------------------------------------\n",
        "# 1.5ë¶€í„° 5.0ê¹Œì§€ 0.1ë‹¨ìœ„ë¡œ ëŠ˜ë ¤ê°€ë©° í™•ì¸\n",
        "for factor in np.arange(1.5, 5.1, 0.1):\n",
        "    temp_indices = []\n",
        "\n",
        "    for col in target_features:\n",
        "        fraud_data = sub_train[sub_train['Class'] == 1][col]\n",
        "        q25 = np.percentile(fraud_data.values, 25)\n",
        "        q75 = np.percentile(fraud_data.values, 75)\n",
        "        iqr = q75 - q25\n",
        "\n",
        "        cut_off = iqr * factor\n",
        "        lower = q25 - cut_off\n",
        "        upper = q75 + cut_off\n",
        "\n",
        "        # ë²”ìœ„ ë²—ì–´ë‚˜ëŠ” ì¸ë±ìŠ¤ ìˆ˜ì§‘\n",
        "        outliers = fraud_data[(fraud_data < lower) | (fraud_data > upper)].index\n",
        "        temp_indices.extend(outliers)\n",
        "\n",
        "    # ì¤‘ë³µ ì œê±°í•œ ì‹¤ì œ ì‚­ì œ ê°œìˆ˜\n",
        "    unique_count = len(set(temp_indices))\n",
        "    loss_rate = unique_count / total_fraud_count\n",
        "\n",
        "    status = \"âœ… ì•ˆì „\" if loss_rate <= max_loss_limit else \"âŒ ìœ„í—˜\"\n",
        "    print(f\"[{factor:.1f} ] | {unique_count:^12} | {loss_rate*100:6.2f}%    | {status}\")\n",
        "\n",
        "    # ì•ˆì „ ë²”ìœ„ì— ë“¤ì–´ì˜¤ëŠ” ì²« ë²ˆì§¸ ê°’(ê°€ì¥ íƒ€ì´íŠ¸í•œ ê°’)ì„ ì°¾ìœ¼ë©´ ì¤‘ë‹¨\n",
        "    if loss_rate <= max_loss_limit:\n",
        "        best_factor = factor\n",
        "        break\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"ğŸ¯ ìµœì¢… ì„ íƒëœ ìµœì  Multiplier: {best_factor:.1f}\")\n",
        "print(f\"   -> ì •ë³´ ì†ì‹¤ì„ {max_loss_limit*100}% ì´ë‚´ë¡œ ë°©ì–´í•˜ë©´ì„œ ê°€ì¥ ê³µê²©ì ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ ì œê±°í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. í™•ì •ëœ Factorë¡œ ì‹¤ì œ ì‚­ì œ ìˆ˜í–‰ ë° ìƒì„¸ ë¦¬í¬íŠ¸\n",
        "# ---------------------------------------------------------\n",
        "final_drop_indices = []\n",
        "report = []\n",
        "\n",
        "for col in target_features:\n",
        "    fraud_data = sub_train[sub_train['Class'] == 1][col]\n",
        "    q25 = np.percentile(fraud_data.values, 25)\n",
        "    q75 = np.percentile(fraud_data.values, 75)\n",
        "    iqr = q75 - q25\n",
        "\n",
        "    cut_off = iqr * best_factor\n",
        "    lower = q25 - cut_off\n",
        "    upper = q75 + cut_off\n",
        "\n",
        "    outliers = fraud_data[(fraud_data < lower) | (fraud_data > upper)].index\n",
        "    final_drop_indices.extend(outliers)\n",
        "\n",
        "    report.append({\n",
        "        'Feature': col,\n",
        "        'Lower_Limit': lower,\n",
        "        'Upper_Limit': upper,\n",
        "        'Outliers_Found': len(outliers)\n",
        "    })\n",
        "\n",
        "# ì¤‘ë³µ ì¸ë±ìŠ¤ ì œê±° í›„ ì‚­ì œ\n",
        "final_drop_indices = list(set(final_drop_indices))\n",
        "sub_train_processed = sub_train.drop(final_drop_indices)\n",
        "\n",
        "# ê²°ê³¼ ë¦¬í¬íŠ¸ ì¶œë ¥\n",
        "print(f\"\\n[ì²˜ë¦¬ ê²°ê³¼]\")\n",
        "print(f\"ì´ ì‚¬ê¸° ë°ì´í„° ìˆ˜: {total_fraud_count}\")\n",
        "print(f\"ì‚­ì œëœ ë°ì´í„° ìˆ˜ : {len(final_drop_indices)}\")\n",
        "print(f\"ìµœì¢… ì •ë³´ ì†ì‹¤ë¥  : {(len(final_drop_indices)/total_fraud_count)*100:.2f}%\")\n",
        "\n",
        "display(pd.DataFrame(report))"
      ],
      "metadata": {
        "id": "rifO5XEVLOLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# 1. ë°ì´í„° ì¤€ë¹„ (ì´ìƒì¹˜ ì œê±°ëœ ë°ì´í„° ì‚¬ìš©)\n",
        "# Class ì»¬ëŸ¼ì„ ì œì™¸í•œ í”¼ì²˜ë“¤(X)ê³¼ Class(y) ë¶„ë¦¬\n",
        "X = sub_train_processed.drop('Class', axis=1)\n",
        "y = sub_train_processed['Class']\n",
        "\n",
        "print(f\"ì‹œê°í™”ì— ì‚¬ìš©í•  ë°ì´í„° í¬ê¸°: {X.shape}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. ì°¨ì› ì¶•ì†Œ ì•Œê³ ë¦¬ì¦˜ ì ìš© (t-SNE, PCA, SVD)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# t-SNE (ê°€ì¥ ì¤‘ìš”)\n",
        "t0 = time.time()\n",
        "X_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\n",
        "t1 = time.time()\n",
        "print(f\"t-SNE ì†Œìš” ì‹œê°„: {t1 - t0:.2f} s\")\n",
        "\n",
        "# PCA\n",
        "t0 = time.time()\n",
        "X_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\n",
        "t1 = time.time()\n",
        "print(f\"PCA ì†Œìš” ì‹œê°„: {t1 - t0:.2f} s\")\n",
        "\n",
        "# Truncated SVD\n",
        "t0 = time.time()\n",
        "X_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)\n",
        "t1 = time.time()\n",
        "print(f\"Truncated SVD ì†Œìš” ì‹œê°„: {t1 - t0:.2f} s\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. ì‹œê°í™” (3ê°œ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ)\n",
        "# ---------------------------------------------------------\n",
        "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 8))\n",
        "# ì œëª© ì„¤ì •\n",
        "f.suptitle('Clusters using Dimensionality Reduction', fontsize=20)\n",
        "\n",
        "blue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\n",
        "red_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n",
        "\n",
        "# 1) t-SNE ì‹œê°í™”\n",
        "ax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
        "ax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n",
        "ax1.set_title('t-SNE', fontsize=16)\n",
        "ax1.grid(True)\n",
        "ax1.legend(handles=[blue_patch, red_patch])\n",
        "\n",
        "# 2) PCA ì‹œê°í™”\n",
        "ax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
        "ax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n",
        "ax2.set_title('PCA', fontsize=16)\n",
        "ax2.grid(True)\n",
        "ax2.legend(handles=[blue_patch, red_patch])\n",
        "\n",
        "# 3) Truncated SVD ì‹œê°í™”\n",
        "ax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
        "ax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n",
        "ax3.set_title('Truncated SVD', fontsize=16)\n",
        "ax3.grid(True)\n",
        "ax3.legend(handles=[blue_patch, red_patch])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bAkyS4AZLOJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 8. ì‹¬í™” í•™ìŠµ: ë¡œì§€ìŠ¤í‹± íšŒê·€ íŠœë‹ (GridSearchCV)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# 1. íŠœë‹í•  íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„¤ì •\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l2'] # l2 ê·œì œ ì‚¬ìš©\n",
        "}\n",
        "\n",
        "# 2. GridSearchCV ê°ì²´ ìƒì„±\n",
        "# scoring='average_precision': AUPRC ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
        "# cv=skf: ì•ì„œ ì •ì˜í•œ StratifiedKFold ê°ì²´ ì‚¬ìš©\n",
        "grid_search = GridSearchCV(\n",
        "    LogisticRegression(max_iter=1000, solver='lbfgs'),\n",
        "    param_grid,\n",
        "    cv=skf,\n",
        "    scoring='average_precision',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 3. í•™ìŠµ ìˆ˜í–‰\n",
        "print(\">>> í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì¤‘ (AUPRC ê¸°ì¤€)...\")\n",
        "grid_search.fit(X_final, y_final)\n",
        "\n",
        "# 4. ìµœì ì˜ íŒŒë¼ë¯¸í„° ë° ì ìˆ˜ ì¶œë ¥\n",
        "print(f\"ìµœì ì˜ íŒŒë¼ë¯¸í„°(C): {grid_search.best_params_}\")\n",
        "print(f\"ìµœê³  AUPRC ì ìˆ˜: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# í˜¼ë™ í–‰ë ¬ (Confusion Matrix) ì‹œê°í™”\n",
        "# ---------------------------------------------------------\n",
        "# ìµœì ì˜ ëª¨ë¸(best_estimator_)ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "# predict()ëŠ” ê¸°ë³¸ ì„ê³„ê°’(0.5)ì„ ê¸°ì¤€ìœ¼ë¡œ 0ê³¼ 1ì„ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_final)\n",
        "\n",
        "# í˜¼ë™ í–‰ë ¬ ê³„ì‚°\n",
        "cm = confusion_matrix(y_final, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['Normal(0)', 'Fraud(1)'],\n",
        "            yticklabels=['Normal(0)', 'Fraud(1)'])\n",
        "plt.title('Confusion Matrix (Optimized by AUPRC)')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "# ìƒì„¸ ë¦¬í¬íŠ¸ ì¶œë ¥\n",
        "print(\"\\n>>> ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ <<<\")\n",
        "print(classification_report(y_final, y_pred))"
      ],
      "metadata": {
        "id": "CsZFEaYrLOGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 9. ìµœì  ì„ê³„ê°’(Threshold) ì°¾ê¸°\n",
        "# ---------------------------------------------------------\n",
        "# 1. í•™ìŠµ ë°ì´í„°(X_final)ì— ëŒ€í•œ ì˜ˆì¸¡ í™•ë¥ (Probability)ì„ êµ¬í•©ë‹ˆë‹¤.\n",
        "# predict_proba ê²°ê³¼ì˜ [:, 1]ì€ 'ì‚¬ê¸°(1)'ì¼ í™•ë¥ ì…ë‹ˆë‹¤.\n",
        "y_prob_train = best_model.predict_proba(X_final)[:, 1]\n",
        "\n",
        "# 2. ì •ë°€ë„(Precision), ì¬í˜„ìœ¨(Recall), ì„ê³„ê°’(Threshold) í›„ë³´ë“¤ì„ ê³„ì‚°\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_final, y_prob_train)\n",
        "\n",
        "# 3. ê° ì„ê³„ê°’ë§ˆë‹¤ F1 Score ê³„ì‚°\n",
        "# F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "# ë¶„ëª¨ê°€ 0ì´ ë˜ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë‚˜ëˆ—ì…ˆ ì•ˆì „ ì²˜ë¦¬\n",
        "with np.errstate(divide='ignore', invalid='ignore'):\n",
        "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
        "    f1_scores = np.nan_to_num(f1_scores) # NaNì„ 0ìœ¼ë¡œ ë³€í™˜\n",
        "\n",
        "# 4. F1 Scoreê°€ ìµœëŒ€ê°€ ë˜ëŠ” ì¸ë±ìŠ¤ì™€ ì„ê³„ê°’ ì°¾ê¸°\n",
        "best_idx = np.argmax(f1_scores)\n",
        "best_threshold = thresholds[best_idx]\n",
        "best_f1 = f1_scores[best_idx]\n",
        "\n",
        "print(f\">>> ìµœì  ì„ê³„ê°’ íƒìƒ‰ ê²°ê³¼ <<<\")\n",
        "print(f\"ìµœì  ì„ê³„ê°’ (Threshold): {best_threshold:.4f}\")\n",
        "print(f\"í•´ë‹¹ ì„ê³„ê°’ì—ì„œì˜ í•™ìŠµ ë°ì´í„° F1 Score: {best_f1:.4f}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 10. Test ë°ì´í„° ì˜ˆì¸¡ ë° Submission ìƒì„±\n",
        "# ---------------------------------------------------------\n",
        "# [ì£¼ì˜] Test ë°ì´í„°ë„ í•™ìŠµ ë°ì´í„°ì™€ ë™ì¼í•œ ì „ì²˜ë¦¬ê°€ ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤!\n",
        "# X_finalì— ìˆëŠ” ì»¬ëŸ¼ë§Œ test ë°ì´í„°ì—ì„œ ì„ íƒ (ìˆœì„œ ë³´ì¥)\n",
        "# ë§Œì•½ test ë°ì´í„°ì— 'ID' ê°™ì€ ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì œì™¸ë©ë‹ˆë‹¤.\n",
        "X_test = test[X_final.columns].copy()\n",
        "\n",
        "# 1. Test ë°ì´í„° ì˜ˆì¸¡ í™•ë¥  êµ¬í•˜ê¸°\n",
        "y_prob_test = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# 2. ìµœì  ì„ê³„ê°’ ì ìš©í•˜ì—¬ 0, 1ë¡œ ë³€í™˜\n",
        "final_preds = (y_prob_test >= best_threshold).astype(int)\n",
        "\n",
        "# 3. Submission íŒŒì¼ ë§Œë“¤ê¸°\n",
        "submission['Class'] = final_preds\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"\\n>>> submission.csv ìƒì„± ì™„ë£Œ! <<<\")\n",
        "print(submission['Class'].value_counts()) # 0ê³¼ 1ì˜ ë¶„í¬ í™•ì¸"
      ],
      "metadata": {
        "id": "ybaQeuxULOE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import precision_recall_curve, f1_score\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# [ìˆ˜ì •] ì§„ì§œ ì›ë³¸ ë°ì´í„° ë¡œë“œ ë° ì„ê³„ê°’ ì°¾ê¸°\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# 1. ì›ë³¸ train ë°ì´í„° ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ê¸° (ê²½ë¡œ í™•ì¸ í•„ìš”)\n",
        "full_train_df = train.copy() # í˜¹ì€ ê¸°ì¡´ì— ë¡œë“œí•´ë‘” ì›ë³¸ ë³€ìˆ˜ ì‚¬ìš©\n",
        "\n",
        "# 2. ì „ì²˜ë¦¬ (ë§¤ìš° ì¤‘ìš”: í•™ìŠµ ë•Œì™€ ë˜‘ê°™ì´ í•´ì¤˜ì•¼ í•¨)\n",
        "# 'ID' ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì œê±° (í•™ìŠµì— ì•ˆ ì¼ìœ¼ë‹ˆê¹Œ)\n",
        "if 'id' in full_train_df.columns:\n",
        "    full_train_df = full_train_df.drop('id', axis=1)\n",
        "\n",
        "# X, y ë¶„ë¦¬\n",
        "X_full_real = full_train_df.drop('Class', axis=1)\n",
        "y_full_real = full_train_df['Class']\n",
        "\n",
        "# [ì¤‘ìš”] ìŠ¤ì¼€ì¼ë§ ì ìš©\n",
        "# í•™ìŠµí•  ë•Œ ì‚¬ìš©í–ˆë˜ ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ëŸ¬(scaler) ê°ì²´ê°€ ë©”ëª¨ë¦¬ì— ìˆë‹¤ë©´ ê·¸ê±¸ë¡œ transform í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "# ë§Œì•½ scaler ê°ì²´ ë³€ìˆ˜ëª…ì´ 'scaler'ë¼ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ í›„ ì‹¤í–‰:\n",
        "# X_full_real[X_full_real.columns] = scaler.transform(X_full_real)\n",
        "\n",
        "# â€» ë§Œì•½ scaler ê°ì²´ê°€ ì—†ë‹¤ë©´, testì…‹ ë§Œë“¤ ë•Œ ì¼ë˜ ë°©ì‹ ê·¸ëŒ€ë¡œ ìŠ¤ì¼€ì¼ë§ì„ ë‹¤ì‹œ í•´ì¤˜ì•¼ í•©ë‹ˆë‹¤.\n",
        "# (ì—¬ê¸°ì„œëŠ” ì‘ì„±ìë‹˜ì´ ì´ë¯¸ testì…‹ì— ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§ì„ í–ˆë‹¤ê³  í–ˆìœ¼ë¯€ë¡œ,\n",
        "#  ì›ë³¸ trainì—ë„ ë˜‘ê°™ì´ ì ìš©ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜, ì•„ë˜ì²˜ëŸ¼ ì§ì ‘ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤.)\n",
        "# from sklearn.preprocessing import RobustScaler\n",
        "# rob_scaler = RobustScaler()\n",
        "# X_full_real[col_names] = rob_scaler.fit_transform(X_full_real[col_names])\n",
        "# -> ì£¼ì˜: ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ì´ ë§ì•„ì•¼ í•¨.\n",
        "# -> ê°€ì¥ ì¢‹ì€ ê±´ í•™ìŠµ ë•Œ ì“´ scaler.transform(X_full_real) ì…ë‹ˆë‹¤.\n",
        "\n",
        "# í•™ìŠµ ë°ì´í„° ì»¬ëŸ¼ ìˆœì„œ ë§ì¶”ê¸° (ì•ˆì „ì¥ì¹˜)\n",
        "X_full_real = X_full_real[X_final.columns]\n",
        "\n",
        "print(f\"ì§„ì§œ ì›ë³¸ ë°ì´í„° í¬ê¸°: {X_full_real.shape}\")\n",
        "print(f\"ì§„ì§œ ì›ë³¸ ì‚¬ê¸° ë¹„ìœ¨: {y_full_real.mean() * 100:.4f}% (0.2% ê·¼ì²˜ì—¬ì•¼ í•¨)\")\n",
        "\n",
        "# 3. ëª¨ë¸ ì˜ˆì¸¡ (ì „ì²´ ë°ì´í„° ëŒ€ìƒ)\n",
        "# ëª¨ë¸ì€ 50:50ìœ¼ë¡œ í•™ìŠµí–ˆì§€ë§Œ, ì˜ˆì¸¡ì€ 28ë§Œ ê±´ ì „ì²´ì— ëŒ€í•´ ìˆ˜í–‰\n",
        "y_prob_full_real = best_model.predict_proba(X_full_real)[:, 1]\n",
        "\n",
        "# 4. ì •ë°€ë„-ì¬í˜„ìœ¨ ê³¡ì„ ìœ¼ë¡œ ìµœì  ì„ê³„ê°’ ì°¾ê¸°\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_full_real, y_prob_full_real)\n",
        "\n",
        "with np.errstate(divide='ignore', invalid='ignore'):\n",
        "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
        "    f1_scores = np.nan_to_num(f1_scores)\n",
        "\n",
        "best_idx = np.argmax(f1_scores)\n",
        "best_threshold_final = thresholds[best_idx]\n",
        "best_f1_final = f1_scores[best_idx]\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\">>> ìµœì¢… ë³´ì •ëœ ì„ê³„ê°’ ê²°ê³¼ <<<\")\n",
        "print(f\"ê¸°ì¡´ ì„ê³„ê°’: {best_threshold:.4f}\")\n",
        "print(f\"ìµœì¢… ì„ê³„ê°’: {best_threshold_final:.4f} (ì´ ê°’ì´ 0.9 ì´ìƒ ë‚˜ì™€ì•¼ ì •ìƒ)\")\n",
        "print(f\"ì˜ˆìƒ F1 Score: {best_f1_final:.4f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Submission ìƒì„± (ìµœì¢…)\n",
        "# ---------------------------------------------------------\n",
        "# Test ë°ì´í„° ì˜ˆì¸¡ê°’ (ì´ë¯¸ êµ¬í•´ë‘” y_prob_test ì‚¬ìš© ê°€ëŠ¥)\n",
        "# í˜¹ì‹œ ëª¨ë¥´ë‹ˆ ë‹¤ì‹œ ê³„ì‚°\n",
        "X_test = test[X_final.columns]\n",
        "y_prob_test = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# ìµœì¢… ì„ê³„ê°’ ì ìš©\n",
        "final_preds_submission = (y_prob_test >= best_threshold_final).astype(int)\n",
        "\n",
        "submission['Class'] = final_preds_submission\n",
        "submission.to_csv('submission_final.csv', index=False)\n",
        "\n",
        "print(\">>> submission_final.csv ìƒì„± ì™„ë£Œ.\")\n",
        "print(f\"ì˜ˆì¸¡ëœ ì‚¬ê¸° ê±°ë˜ ìˆ˜: {submission['Class'].sum()}\")"
      ],
      "metadata": {
        "id": "DnkA1B9RLOCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. ë°ì´í„° ì¤€ë¹„ (fraud_final í™œìš©)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# 1) ì‚¬ê¸° ë°ì´í„°: ì‚¬ìš©ìê°€ ì§€ì •í•œ 'fraud_final' ì‚¬ìš© (ì´ìƒì¹˜ ì œê±°ë¨)\n",
        "X_fraud = fraud_final.drop('Class', axis=1)\n",
        "y_fraud = fraud_final['Class']\n",
        "\n",
        "# 2) ì •ìƒ ë°ì´í„°: ì „ì²´ ì›ë³¸ ë°ì´í„°(X_full_real)ì—ì„œ ê°€ì ¸ì˜¤ê¸°\n",
        "# (X_full_real, y_full_realì€ ì§ì „ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ ì›ë³¸ ìŠ¤ì¼€ì¼ë§ ë°ì´í„°ì…ë‹ˆë‹¤)\n",
        "X_normal_all = X_full_real[y_full_real == 0]\n",
        "y_normal_all = y_full_real[y_full_real == 0]\n",
        "\n",
        "# [ì¤‘ìš”] ì»¬ëŸ¼ ìˆœì„œ ë° ì¢…ë¥˜ ë™ê¸°í™” (ì—ëŸ¬ ë°©ì§€)\n",
        "X_normal_all = X_normal_all[X_fraud.columns]\n",
        "\n",
        "# 3) ê°œìˆ˜ ìë™ ê³„ì‚°\n",
        "n_fraud = len(X_fraud)          # ì •ì œëœ ì‚¬ê¸° ë°ì´í„° ê°œìˆ˜\n",
        "n_normal = len(X_normal_all)    # ì „ì²´ ì •ìƒ ë°ì´í„° ê°œìˆ˜\n",
        "n_bags = n_normal // n_fraud    # ë§Œë“¤ ëª¨ë¸ ê°œìˆ˜ (ëª«)\n",
        "\n",
        "print(f\"ì‚¬ìš©í•  ì‚¬ê¸° ë°ì´í„° ìˆ˜(n_fraud): {n_fraud}\")\n",
        "print(f\"ì „ì²´ ì •ìƒ ë°ì´í„° ìˆ˜: {n_normal}\")\n",
        "print(f\">>> ì´ {n_bags}ê°œì˜ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤ (ì •ìƒ ë°ì´í„°ë¥¼ {n_bags}ë“±ë¶„).\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. EasyEnsemble í•™ìŠµ Loop\n",
        "# ---------------------------------------------------------\n",
        "# ì •ìƒ ë°ì´í„° ì„ê¸° (ëœë¤ì„±ì„ ìœ„í•´)\n",
        "X_normal_shuffled = X_normal_all.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "models = [] # í•™ìŠµëœ ëª¨ë¸ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
        "\n",
        "print(\"\\n>>> í•™ìŠµ ì‹œì‘...\")\n",
        "for i in range(n_bags):\n",
        "    # 1. ì •ìƒ ë°ì´í„° ìŠ¬ë¼ì´ì‹± (ë¹„ë³µì› ì¶”ì¶œ)\n",
        "    # 0~n, n~2n, 2n~3n ... ìˆœì„œëŒ€ë¡œ ê°€ì ¸ì˜´\n",
        "    start = i * n_fraud\n",
        "    end = (i + 1) * n_fraud\n",
        "\n",
        "    X_normal_subset = X_normal_shuffled.iloc[start:end]\n",
        "    y_normal_subset = pd.Series([0] * len(X_normal_subset)) # ë¼ë²¨ 0 ìƒì„±\n",
        "\n",
        "    # 2. í•™ìŠµ ë°ì´í„°ì…‹ ê²°í•© (Clean Fraud + Random Normal)\n",
        "    X_train_bag = pd.concat([X_fraud, X_normal_subset])\n",
        "    y_train_bag = pd.concat([y_fraud, y_normal_subset])\n",
        "\n",
        "    # 3. ëª¨ë¸ í•™ìŠµ\n",
        "    model = LogisticRegression(max_iter=1000, solver='lbfgs', n_jobs=-1)\n",
        "    model.fit(X_train_bag, y_train_bag)\n",
        "\n",
        "    models.append(model)\n",
        "\n",
        "    # ì§„í–‰ ìƒí™© ì¶œë ¥ (50ê°œ ë‹¨ìœ„)\n",
        "    if (i + 1) % 50 == 0:\n",
        "        print(f\"[{i + 1}/{n_bags}] ëª¨ë¸ í•™ìŠµ ì¤‘...\")\n",
        "\n",
        "print(\">>> ëª¨ë“  ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. ì•™ìƒë¸” í™•ë¥  ê³„ì‚° (ì „ì²´ ë°ì´í„° ëŒ€ìƒ)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n>>> ìµœì  ì„ê³„ê°’ì„ ì°¾ê¸° ìœ„í•´ ì „ì²´ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...\")\n",
        "\n",
        "# ì „ì²´ ë°ì´í„°(X_full_real)ì— ëŒ€í•´ ëª¨ë“  ëª¨ë¸ì˜ í™•ë¥ ì„ ë”í•¨\n",
        "probs_sum = np.zeros(len(X_full_real))\n",
        "\n",
        "# X_full_realì˜ ì»¬ëŸ¼ ìˆœì„œë„ X_fraudì™€ ë§ì¶°ì¤Œ (ì•ˆì „ì¥ì¹˜)\n",
        "X_full_for_predict = X_full_real[X_fraud.columns]\n",
        "\n",
        "for model in models:\n",
        "    probs_sum += model.predict_proba(X_full_for_predict)[:, 1]\n",
        "\n",
        "# í‰ê·  í™•ë¥  ê³„ì‚°\n",
        "y_prob_ensemble = probs_sum / n_bags\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. ìµœì  ì„ê³„ê°’(Threshold) ì°¾ê¸°\n",
        "# ---------------------------------------------------------\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_full_real, y_prob_ensemble)\n",
        "\n",
        "with np.errstate(divide='ignore', invalid='ignore'):\n",
        "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
        "    f1_scores = np.nan_to_num(f1_scores)\n",
        "\n",
        "best_idx = np.argmax(f1_scores)\n",
        "best_threshold_ee = thresholds[best_idx]\n",
        "best_f1_ee = f1_scores[best_idx]\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\">>> EasyEnsemble ê²°ê³¼ <<<\")\n",
        "print(f\"ìµœì  ì„ê³„ê°’: {best_threshold_ee:.4f}\")\n",
        "print(f\"ì˜ˆìƒ F1 Score: {best_f1_ee:.4f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. Submission ìƒì„±\n",
        "# ---------------------------------------------------------\n",
        "print(\">>> Test ë°ì´í„° ì˜ˆì¸¡ ë° íŒŒì¼ ìƒì„± ì¤‘...\")\n",
        "\n",
        "X_test = test[X_fraud.columns] # ì»¬ëŸ¼ ìˆœì„œ ë§ì¶”ê¸°\n",
        "\n",
        "# Test ë°ì´í„°ì— ëŒ€í•´ ì•™ìƒë¸” ì˜ˆì¸¡\n",
        "test_probs_sum = np.zeros(len(X_test))\n",
        "\n",
        "for model in models:\n",
        "    test_probs_sum += model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "y_prob_test_ee = test_probs_sum / n_bags\n",
        "\n",
        "# ìµœì  ì„ê³„ê°’ ì ìš©\n",
        "final_preds_ee = (y_prob_test_ee >= best_threshold_ee).astype(int)\n",
        "\n",
        "submission['Class'] = final_preds_ee\n",
        "submission.to_csv('submission_easy_ensemble.csv', index=False)\n",
        "\n",
        "print(\">>> submission_easy_ensemble.csv ìƒì„± ì™„ë£Œ.\")\n",
        "print(f\"ì˜ˆì¸¡ëœ ì‚¬ê¸° ê±°ë˜ ìˆ˜: {submission['Class'].sum()}\")"
      ],
      "metadata": {
        "id": "HFTLCe35LOAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# ë¡œì§€ìŠ¤í‹± íšŒê·€ EasyEnsemble: ì„ê³„ê°’ ìµœì í™” ë° ì œì¶œ íŒŒì¼ ì¬ìƒì„±\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# 1. ì „ì²´ í•™ìŠµ ë°ì´í„°(X_full_real)ì— ëŒ€í•œ ì•™ìƒë¸” í™•ë¥ ê°’ ê³„ì‚°\n",
        "print(\">>> [ë¡œì§€ìŠ¤í‹± íšŒê·€] ì „ì²´ ë°ì´í„° í™•ë¥  ê³„ì‚° ë° ì„ê³„ê°’ ìµœì í™” ì‹œì‘...\")\n",
        "\n",
        "# ì»¬ëŸ¼ ìˆœì„œ ê°•ì œ ì¼ì¹˜ (í•™ìŠµ ë°ì´í„° ê¸°ì¤€)\n",
        "X_full_opt = X_full_real[X_fraud.columns]\n",
        "\n",
        "# ê¸°ì¡´ì— í•™ìŠµëœ models ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©\n",
        "probs_sum_train = np.zeros(len(X_full_opt))\n",
        "\n",
        "for model in models:\n",
        "    probs_sum_train += model.predict_proba(X_full_opt)[:, 1]\n",
        "\n",
        "# í‰ê·  í™•ë¥  (Soft Voting)\n",
        "y_prob_ensemble_train = probs_sum_train / len(models)\n",
        "\n",
        "# 2. ì •ë°€ë„-ì¬í˜„ìœ¨ ê³¡ì„ ì„ í†µí•´ ìµœì  ì„ê³„ê°’ ì°¾ê¸°\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_full_real, y_prob_ensemble_train)\n",
        "\n",
        "# F1 Score ê³„ì‚°\n",
        "with np.errstate(divide='ignore', invalid='ignore'):\n",
        "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
        "    f1_scores = np.nan_to_num(f1_scores)\n",
        "\n",
        "# ìµœì  ìœ„ì¹˜ ì°¾ê¸°\n",
        "best_idx = np.argmax(f1_scores)\n",
        "best_threshold_lr = thresholds[best_idx]\n",
        "best_f1_lr = f1_scores[best_idx]\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\">>> ë¡œì§€ìŠ¤í‹± EasyEnsemble ìµœì í™” ê²°ê³¼ <<<\")\n",
        "print(f\"ìµœì  ì„ê³„ê°’: {best_threshold_lr:.4f}\")\n",
        "print(f\"ì˜ˆìƒ F1 Score: {best_f1_lr:.4f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 3. Test ë°ì´í„°ì— ì ìš© ë° Submission ì €ì¥\n",
        "print(\">>> Test ë°ì´í„° ì˜ˆì¸¡ ê°’ì— ìµœì  ì„ê³„ê°’ ì ìš© ì¤‘...\")\n",
        "\n",
        "# Test ë°ì´í„° ì¤€ë¹„\n",
        "X_test_lr = test[X_fraud.columns]\n",
        "\n",
        "# Test ë°ì´í„° í™•ë¥  ê³„ì‚° (ì´ë¯¸ ê³„ì‚°ëœ ê²Œ ë³€ìˆ˜ì— ìˆë‹¤ë©´ ê·¸ê±¸ ì¨ë„ ë˜ì§€ë§Œ, ì•ˆì „í•˜ê²Œ ë‹¤ì‹œ ê³„ì‚°)\n",
        "probs_sum_test = np.zeros(len(X_test_lr))\n",
        "\n",
        "for model in models:\n",
        "    probs_sum_test += model.predict_proba(X_test_lr)[:, 1]\n",
        "\n",
        "y_prob_ensemble_test = probs_sum_test / len(models)\n",
        "\n",
        "# ìµœì  ì„ê³„ê°’ ì ìš©í•˜ì—¬ 0, 1 ìƒì„±\n",
        "final_preds_lr = (y_prob_ensemble_test >= best_threshold_lr).astype(int)\n",
        "\n",
        "# ì €ì¥\n",
        "submission['Class'] = final_preds_lr\n",
        "submission.to_csv('submission_easy_ensemble_lr_optimized.csv', index=False)\n",
        "\n",
        "print(\">>> submission_easy_ensemble_lr_optimized.csv ìƒì„± ì™„ë£Œ.\")\n",
        "print(f\"ìµœì¢… ì˜ˆì¸¡ëœ ì‚¬ê¸° ê±°ë˜ ìˆ˜: {submission['Class'].sum()}\")"
      ],
      "metadata": {
        "id": "rSW_1NAWLN91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import precision_recall_curve, f1_score\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. ë°ì´í„° ì¤€ë¹„ (ì „ì²˜ë¦¬ ì™„ë£Œëœ ë°ì´í„° ë¡œë“œ)\n",
        "# ---------------------------------------------------------\n",
        "# 1) ì‚¬ê¸° ë°ì´í„°: ì •ì œëœ sub_train_processed ì‚¬ìš©\n",
        "fraud_data = sub_train_processed[sub_train_processed['Class'] == 1]\n",
        "X_fraud = fraud_data.drop('Class', axis=1)\n",
        "y_fraud = fraud_data['Class']\n",
        "\n",
        "# 2) ì •ìƒ ë°ì´í„°: ì „ì²´ ì›ë³¸(X_full_real)ì—ì„œ ê°€ì ¸ì˜´\n",
        "X_normal_all = X_full_real[y_full_real == 0]\n",
        "y_normal_all = y_full_real[y_full_real == 0]\n",
        "\n",
        "# [ì¤‘ìš”] ì»¬ëŸ¼ ìˆœì„œ ê°•ì œ ë™ê¸°í™”\n",
        "X_normal_all = X_normal_all[X_fraud.columns]\n",
        "\n",
        "# 3) Bagging ê°œìˆ˜ ê³„ì‚°\n",
        "n_fraud = len(X_fraud)\n",
        "n_bags = len(X_normal_all) // n_fraud\n",
        "\n",
        "print(f\">>> [ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ]\")\n",
        "print(f\" - ì‚¬ê¸° ë°ì´í„° ìˆ˜: {n_fraud}\")\n",
        "print(f\" - ì •ìƒ ë°ì´í„° ìˆ˜: {len(X_normal_all)}\")\n",
        "print(f\" - ìƒì„±í•  ëª¨ë¸(Bags) ìˆ˜: {n_bags}ê°œ\")\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold # ì´ê±° import í•„ìˆ˜\n",
        "\n",
        "# ... (ì•ë¶€ë¶„ ë°ì´í„° ì¤€ë¹„ ì½”ë“œëŠ” ë™ì¼) ...\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. ì •ë°€ íŠœë‹ (GridSearchCV + StratifiedKFold ëª…ì‹œ)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n>>> [1ë‹¨ê³„] ìµœì ì˜ íŒŒë¼ë¯¸í„° íƒìƒ‰ (GridSearchCV + Stratified K-Fold)...\")\n",
        "\n",
        "# íŠœë‹ìš© ëŒ€í‘œ ë°ì´í„°ì…‹ (1:1 ë¹„ìœ¨)\n",
        "X_normal_sample = X_normal_all.sample(n=n_fraud, random_state=42)\n",
        "y_normal_sample = pd.Series([0] * n_fraud)\n",
        "\n",
        "X_tune = pd.concat([X_fraud, X_normal_sample])\n",
        "y_tune = pd.concat([y_fraud, y_normal_sample])\n",
        "\n",
        "# [ìˆ˜ì •ë¨] ì¸µí™” Kí´ë“œ ê°ì²´ ëª…ì‹œì  ìƒì„± (ì„ê¸° í¬í•¨)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300, 500],\n",
        "    'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
        "    'num_leaves': [31, 63, 127],\n",
        "    'max_depth': [5, 10, 20, -1],\n",
        "    'min_child_samples': [10, 20, 30]\n",
        "}\n",
        "\n",
        "lgbm = LGBMClassifier(random_state=42, verbose=-1, n_jobs=1)\n",
        "\n",
        "# GridSearchCV ì‹¤í–‰\n",
        "grid_search = GridSearchCV(\n",
        "    lgbm,\n",
        "    param_grid,\n",
        "    cv=skf,               # [ìˆ˜ì •ë¨] ìˆ«ì 5 ëŒ€ì‹  skf ê°ì²´ ì „ë‹¬ -> í™•ì‹¤í•˜ê²Œ ì¸µí™”+ì„ê¸° ì ìš©\n",
        "    scoring='average_precision',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_tune, y_tune)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"\\nâœ… ìµœì  íŒŒë¼ë¯¸í„° ë°œê²¬: {best_params}\")\n",
        "print(f\"âœ… ìµœê³  AUPRC ì ìˆ˜: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# ... (ì´í›„ EasyEnsemble í•™ìŠµ ë° ì„ê³„ê°’ ìµœì í™” ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ì‹¤í–‰) ...\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. EasyEnsemble í•™ìŠµ (ìµœì  íŒŒë¼ë¯¸í„° ì ìš©)\n",
        "# ---------------------------------------------------------\n",
        "print(f\"\\n>>> [2ë‹¨ê³„] EasyEnsemble í•™ìŠµ ì‹œì‘ ({n_bags}ê°œ ëª¨ë¸)...\")\n",
        "\n",
        "# ì •ìƒ ë°ì´í„° ì„ê¸°\n",
        "X_normal_shuffled = X_normal_all.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "models = []\n",
        "\n",
        "for i in range(n_bags):\n",
        "    start = i * n_fraud\n",
        "    end = (i + 1) * n_fraud\n",
        "\n",
        "    # ì •ìƒ ë°ì´í„° ìŠ¬ë¼ì´ì‹±\n",
        "    X_normal_subset = X_normal_shuffled.iloc[start:end]\n",
        "    y_normal_subset = pd.Series([0] * len(X_normal_subset))\n",
        "\n",
        "    # ë°ì´í„° ê²°í•©\n",
        "    X_train_bag = pd.concat([X_fraud, X_normal_subset])\n",
        "    y_train_bag = pd.concat([y_fraud, y_normal_subset])\n",
        "\n",
        "    # [í•µì‹¬] ì°¾ì€ ìµœì  íŒŒë¼ë¯¸í„°(**best_params) ì ìš©\n",
        "    model = LGBMClassifier(**best_params, random_state=42, verbose=-1, n_jobs=1)\n",
        "    model.fit(X_train_bag, y_train_bag)\n",
        "    models.append(model)\n",
        "\n",
        "    if (i + 1) % 50 == 0:\n",
        "        print(f\" - [{i + 1}/{n_bags}] ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\")\n",
        "\n",
        "print(\">>> ëª¨ë“  ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. ì•™ìƒë¸” í™•ë¥  ê³„ì‚° (Soft Voting)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n>>> [3ë‹¨ê³„] ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ì•™ìƒë¸” í™•ë¥  ê³„ì‚° ì¤‘...\")\n",
        "\n",
        "# ì „ì²´ ì›ë³¸ ë°ì´í„°(X_full_real) ì»¬ëŸ¼ ë§ì¶”ê¸°\n",
        "X_full_predict = X_full_real[X_fraud.columns]\n",
        "\n",
        "# ëˆ„ì  í™•ë¥  ë³€ìˆ˜\n",
        "probs_sum = np.zeros(len(X_full_predict))\n",
        "\n",
        "# ëª¨ë“  ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ í•©ì‚°\n",
        "for model in models:\n",
        "    probs_sum += model.predict_proba(X_full_predict)[:, 1]\n",
        "\n",
        "# í‰ê·  ê³„ì‚°\n",
        "y_prob_ensemble = probs_sum / n_bags\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. ìµœì¢… ì„ê³„ê°’ ìµœì í™” (F1 Score ê¸°ì¤€)\n",
        "# ---------------------------------------------------------\n",
        "print(\">>> [4ë‹¨ê³„] ìµœê³  F1 ì ìˆ˜ë¥¼ ìœ„í•œ ì„ê³„ê°’(Threshold) ì°¾ëŠ” ì¤‘...\")\n",
        "\n",
        "# Precision-Recall Curve ê³„ì‚°\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_full_real, y_prob_ensemble)\n",
        "\n",
        "# F1 Score ê³„ì‚°\n",
        "with np.errstate(divide='ignore', invalid='ignore'):\n",
        "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
        "    f1_scores = np.nan_to_num(f1_scores)\n",
        "\n",
        "# ìµœëŒ€ F1 ìœ„ì¹˜ ì°¾ê¸°\n",
        "best_idx = np.argmax(f1_scores)\n",
        "best_threshold = thresholds[best_idx]\n",
        "best_f1 = f1_scores[best_idx]\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"ğŸ† ìµœì¢… ìµœì í™” ê²°ê³¼ ğŸ†\")\n",
        "print(f\"1. í•™ìŠµ ëª¨ë¸: LightGBM EasyEnsemble (GridSearch AUPRC)\")\n",
        "print(f\"2. ìµœì  ì„ê³„ê°’: {best_threshold:.6f}\")\n",
        "print(f\"3. ì˜ˆìƒ ìµœê³  F1 Score: {best_f1:.6f}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 6. Submission íŒŒì¼ ìƒì„±\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n>>> [5ë‹¨ê³„] Test ë°ì´í„° ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±...\")\n",
        "\n",
        "# Test ë°ì´í„° ì»¬ëŸ¼ ì •ë ¬\n",
        "X_test = test[X_fraud.columns]\n",
        "\n",
        "# Test ë°ì´í„° í™•ë¥  ê³„ì‚° (ì•™ìƒë¸”)\n",
        "test_probs_sum = np.zeros(len(X_test))\n",
        "\n",
        "for model in models:\n",
        "    test_probs_sum += model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "y_prob_test_ensemble = test_probs_sum / n_bags\n",
        "\n",
        "# ìµœì  ì„ê³„ê°’ ì ìš©\n",
        "final_preds = (y_prob_test_ensemble >= best_threshold).astype(int)\n",
        "\n",
        "# íŒŒì¼ ì €ì¥\n",
        "submission['Class'] = final_preds\n",
        "filename = 'submission_lgbm_ensemble_grid_auprc.csv'\n",
        "submission.to_csv(filename, index=False)\n",
        "\n",
        "print(f\">>> {filename} ìƒì„± ì™„ë£Œ.\")\n",
        "print(f\"ì˜ˆì¸¡ëœ ì‚¬ê¸° ê±°ë˜ ìˆ˜: {submission['Class'].sum()}\")\n",
        "\n",
        "# ì†Œìš”ì‹œê°„ 3795ì´ˆ"
      ],
      "metadata": {
        "id": "CN2JefpxLN7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# 1. ì›ë³¸ ë°ì´í„° ë¡œë“œ\n",
        "full_df = pd.read_csv('train.csv')\n",
        "\n",
        "# 2. ì „ì²˜ë¦¬ (í•™ìŠµ ë°ì´í„°ì™€ ë™ì¼í•˜ê²Œ ë§Œë“¤ê¸°)\n",
        "# 'ID' ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì œê±°\n",
        "if 'id' in full_df.columns:\n",
        "    full_df.drop('id', axis=1, inplace=True)\n",
        "\n",
        "# 3. Robust Scaler ì ìš© ('Time', 'Amount')\n",
        "# ìƒˆë¡œìš´ ì»¬ëŸ¼ ìƒì„±\n",
        "rob_scaler = RobustScaler()\n",
        "full_df['scaled_amount'] = rob_scaler.fit_transform(full_df['Amount'].values.reshape(-1,1))\n",
        "full_df['scaled_time'] = rob_scaler.fit_transform(full_df['Time'].values.reshape(-1,1))\n",
        "\n",
        "# ê¸°ì¡´ ì»¬ëŸ¼ ì‚­ì œ\n",
        "full_df.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
        "\n",
        "# 4. X(ë°ì´í„°), y(ë¼ë²¨) ë¶„ë¦¬\n",
        "X_full_real = full_df.drop('Class', axis=1)\n",
        "y_full_real = full_df['Class']\n",
        "\n",
        "# [ì¤‘ìš”] ì»¬ëŸ¼ ìˆœì„œë¥¼ sub_train_processedì™€ ê°•ì œë¡œ ë§ì¶¤ (ì—ëŸ¬ ë°©ì§€)\n",
        "# sub_train_processedì—ì„œ Classë¥¼ ì œì™¸í•œ ì»¬ëŸ¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
        "feature_cols = sub_train_processed.drop('Class', axis=1).columns\n",
        "X_full_real = X_full_real[feature_cols]\n",
        "\n",
        "print(\">>> ì›ë³¸ ì „ì²´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ\")\n",
        "print(f\"X_full_real í¬ê¸°: {X_full_real.shape} (ì•½ 17ë§Œ ê±´ì´ì–´ì•¼ ì •ìƒ)\")\n",
        "print(f\"y_full_real í¬ê¸°: {y_full_real.shape}\")"
      ],
      "metadata": {
        "id": "95waPY8_LN5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.metrics import precision_recall_curve, f1_score, average_precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. ë°ì´í„° ì¤€ë¹„\n",
        "# ---------------------------------------------------------\n",
        "# ì‚¬ê¸°(Fraud) ë°ì´í„°ì™€ ì •ìƒ(Normal) ë°ì´í„° ë¶„ë¦¬\n",
        "# (sub_train_processed, X_full_real, y_full_real, test ë³€ìˆ˜ê°€ ë©”ëª¨ë¦¬ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤)\n",
        "\n",
        "X_fraud = sub_train_processed[sub_train_processed['Class'] == 1].drop('Class', axis=1)\n",
        "y_fraud = sub_train_processed[sub_train_processed['Class'] == 1]['Class']\n",
        "\n",
        "X_normal_all = X_full_real[y_full_real == 0]\n",
        "# ì»¬ëŸ¼ ìˆœì„œ ë™ê¸°í™”\n",
        "X_normal_all = X_normal_all[X_fraud.columns]\n",
        "\n",
        "print(f\"ì‚¬ê¸° ë°ì´í„°: {len(X_fraud)}\")\n",
        "print(f\"ì •ìƒ ë°ì´í„°: {len(X_normal_all)}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Balanced Cascade í•™ìŠµ ë¡œì§ (Hard Negative Mining)\n",
        "# ---------------------------------------------------------\n",
        "models = []\n",
        "n_stages = 10  # ëª¨ë¸ ê°œìˆ˜\n",
        "current_normal_pool = X_normal_all.copy() # ì •ìƒ ë°ì´í„° í’€\n",
        "\n",
        "print(f\"\\n>>> Balanced Cascade (CatBoost + AUPRC) í•™ìŠµ ì‹œì‘...\")\n",
        "\n",
        "for i in range(n_stages):\n",
        "    # 1) ì •ìƒ ë°ì´í„° ì–¸ë”ìƒ˜í”Œë§ (ì‚¬ê¸° ë°ì´í„° ê°œìˆ˜ë§Œí¼)\n",
        "    if len(current_normal_pool) < len(X_fraud):\n",
        "        print(f\"  [Info] ë‚¨ì€ ì •ìƒ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ {i}ë‹¨ê³„ì—ì„œ ì¡°ê¸° ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
        "        break\n",
        "\n",
        "    # ëœë¤í•˜ê²Œ ë½‘ë˜, random_stateë¥¼ ë°”ê¿”ê°€ë©° ë‹¤ì–‘ì„± í™•ë³´\n",
        "    X_normal_sample = current_normal_pool.sample(n=len(X_fraud), random_state=42+i)\n",
        "    y_normal_sample = pd.Series([0] * len(X_normal_sample))\n",
        "\n",
        "    # 2) í•™ìŠµ ë°ì´í„° ìƒì„± (1:1 ë¹„ìœ¨)\n",
        "    X_train = pd.concat([X_fraud, X_normal_sample])\n",
        "    y_train = pd.concat([y_fraud, y_normal_sample])\n",
        "\n",
        "    # 3) CatBoost ëª¨ë¸ í•™ìŠµ\n",
        "    # [ìˆ˜ì •] eval_metric='PRAUC'ë¡œ ì„¤ì •í•˜ì—¬ í•™ìŠµ ê³¼ì •ì—ì„œ AUPRC ìµœì í™”\n",
        "    model = CatBoostClassifier(\n",
        "        iterations=1000,\n",
        "        learning_rate=0.05,\n",
        "        depth=6,\n",
        "        eval_metric='PRAUC',  # AUPRC (Precision-Recall Area Under Curve)\n",
        "        auto_class_weights=None,\n",
        "        verbose=0,\n",
        "        random_seed=42+i,\n",
        "        allow_writing_files=False # ë¶ˆí•„ìš”í•œ íŒŒì¼ ìƒì„± ë°©ì§€\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    models.append(model)\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # [í•µì‹¬] Hard Negative Mining (ì–´ë ¤ìš´ ì •ìƒ ë°ì´í„° ë‚¨ê¸°ê¸°)\n",
        "    # -----------------------------------------------------\n",
        "    if i < n_stages - 1:\n",
        "        # ì „ì²´ í’€ì— ëŒ€í•´ ì˜ˆì¸¡\n",
        "        probs = model.predict_proba(current_normal_pool)[:, 1]\n",
        "\n",
        "        # ëª¨ë¸ì´ 'ì •ìƒ'ì´ë¼ê³  í™•ì‹ í•˜ì§€ ëª»í•˜ëŠ”(ì‚¬ê¸°ì¼ í™•ë¥ ì´ ì¡°ê¸ˆì´ë¼ë„ ìˆëŠ”) ë°ì´í„°ë§Œ ë‚¨ê¹€\n",
        "        # threshold_filter: ì´ ê°’ë³´ë‹¤ í™•ë¥ ì´ ë†’ìœ¼ë©´ \"ì–´ë ¤ìš´ ë°ì´í„°\"ë¡œ ê°„ì£¼\n",
        "        threshold_filter = 0.05\n",
        "        hard_indices = np.where(probs > threshold_filter)[0]\n",
        "\n",
        "        current_normal_pool = current_normal_pool.iloc[hard_indices]\n",
        "\n",
        "        print(f\" - Stage {i+1} ì™„ë£Œ. ë‚¨ì€ 'ì–´ë ¤ìš´' ì •ìƒ ë°ì´í„°: {len(current_normal_pool)}\")\n",
        "\n",
        "        if len(current_normal_pool) < len(X_fraud):\n",
        "            print(\"  -> ë‚¨ì€ ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ ë£¨í”„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
        "            break\n",
        "    else:\n",
        "        print(f\" - Stage {i+1} ì™„ë£Œ (ë§ˆì§€ë§‰ ë‹¨ê³„)\")\n",
        "\n",
        "print(f\">>> ì´ {len(models)}ê°œì˜ Cascade ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. ì•™ìƒë¸” ì˜ˆì¸¡ (Soft Voting)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n>>> ì „ì²´ ë°ì´í„° ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€...\")\n",
        "\n",
        "# ì „ì²´ ì›ë³¸ ë°ì´í„°(X_full_real)ì— ëŒ€í•´ ì˜ˆì¸¡\n",
        "X_full_predict = X_full_real[X_fraud.columns]\n",
        "probs_sum = np.zeros(len(X_full_predict))\n",
        "\n",
        "for model in models:\n",
        "    probs_sum += model.predict_proba(X_full_predict)[:, 1]\n",
        "\n",
        "# í‰ê·  í™•ë¥  ê³„ì‚°\n",
        "y_prob_ensemble = probs_sum / len(models)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. ì„±ëŠ¥ í‰ê°€ ë° Threshold ìµœì í™”\n",
        "# ---------------------------------------------------------\n",
        "# [ìˆ˜ì •] AUPRC ì ìˆ˜ ê³„ì‚° ë° ì¶œë ¥\n",
        "auprc_score = average_precision_score(y_full_real, y_prob_ensemble)\n",
        "print(f\"ğŸ† ì „ì²´ ì•™ìƒë¸” AUPRC Score: {auprc_score:.6f}\")\n",
        "\n",
        "# F1 Score ê¸°ì¤€ ìµœì  ì„ê³„ê°’ ì°¾ê¸° (ì œì¶œìš© 0/1 ìƒì„±ì„ ìœ„í•´ í•„ìˆ˜)\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_full_real, y_prob_ensemble)\n",
        "\n",
        "with np.errstate(divide='ignore', invalid='ignore'):\n",
        "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
        "    f1_scores = np.nan_to_num(f1_scores)\n",
        "\n",
        "best_idx = np.argmax(f1_scores)\n",
        "best_threshold = thresholds[best_idx]\n",
        "best_f1 = f1_scores[best_idx]\n",
        "\n",
        "print(f\"ğŸ† ìµœì  ì„ê³„ê°’ (Max F1): {best_threshold:.6f}\")\n",
        "print(f\"ğŸ† ì˜ˆìƒ ìµœê³  F1 Score: {best_f1:.6f}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. ì œì¶œ íŒŒì¼ ìƒì„±\n",
        "# ---------------------------------------------------------\n",
        "X_test = test[X_fraud.columns]\n",
        "test_probs_sum = np.zeros(len(X_test))\n",
        "\n",
        "for model in models:\n",
        "    test_probs_sum += model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "y_prob_test = test_probs_sum / len(models)\n",
        "final_preds = (y_prob_test >= best_threshold).astype(int)\n",
        "\n",
        "submission['Class'] = final_preds\n",
        "submission.to_csv('submission_catboost_cascade_auprc.csv', index=False)\n",
        "print(\">>> submission_catboost_cascade_auprc.csv ì €ì¥ ì™„ë£Œ.\")\n",
        "print(f\"ì˜ˆì¸¡ëœ ì‚¬ê¸° ê±°ë˜ ìˆ˜: {submission['Class'].sum()}\")\n"
      ],
      "metadata": {
        "id": "nt8TukGJU4hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. ë°ì´í„° ì¤€ë¹„ (ì›ë³¸ ë°ì´í„° X_full_real í•„ìš”)\n",
        "# ---------------------------------------------------------\n",
        "# X_full_real, y_full_realì´ ë©”ëª¨ë¦¬ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "# ì‚¬ê¸° ë°ì´í„° ì¤€ë¹„\n",
        "X_fraud = sub_train_processed[sub_train_processed['Class'] == 1].drop('Class', axis=1)\n",
        "y_fraud = sub_train_processed[sub_train_processed['Class'] == 1]['Class']\n",
        "\n",
        "# ì •ìƒ ë°ì´í„° ì¤€ë¹„\n",
        "X_normal_all = X_full_real[y_full_real == 0]\n",
        "X_normal_all = X_normal_all[X_fraud.columns] # ì»¬ëŸ¼ ìˆœì„œ ë™ê¸°í™”\n",
        "\n",
        "print(f\"ì‚¬ê¸° ë°ì´í„°: {len(X_fraud)}\")\n",
        "print(f\"ì •ìƒ ë°ì´í„°: {len(X_normal_all)}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ëŒ€í‘œ ë°ì´í„°ì…‹ 1ê°œë¡œ ìˆ˜í–‰)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n>>> [1ë‹¨ê³„] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ (RandomizedSearch + AUPRC)...\")\n",
        "\n",
        "# íŠœë‹ìš© ëŒ€í‘œ ë°ì´í„°ì…‹ ìƒì„± (ì‚¬ê¸°:ì •ìƒ = 1:1)\n",
        "X_normal_sample = X_normal_all.sample(n=len(X_fraud), random_state=42)\n",
        "y_normal_sample = pd.Series([0] * len(X_fraud))\n",
        "\n",
        "X_tune = pd.concat([X_fraud, X_normal_sample])\n",
        "y_tune = pd.concat([y_fraud, y_normal_sample])\n",
        "\n",
        "# íƒìƒ‰í•  íŒŒë¼ë¯¸í„° ê³µê°„ ì •ì˜\n",
        "param_dist = {\n",
        "    'iterations': randint(500, 1500),          # íŠ¸ë¦¬ ê°œìˆ˜\n",
        "    'learning_rate': uniform(0.01, 0.2),       # í•™ìŠµë¥ \n",
        "    'depth': randint(4, 10),                   # íŠ¸ë¦¬ ê¹Šì´\n",
        "    'l2_leaf_reg': randint(1, 10),             # L2 ê·œì œ\n",
        "    'random_strength': uniform(0.1, 10),       # ëœë¤ì„± (ê³¼ì í•© ë°©ì§€)\n",
        "    'bagging_temperature': uniform(0.0, 1.0),  # ë°°ê¹… ê°•ë„\n",
        "    'border_count': [32, 64, 128, 254]         # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í•  ìˆ˜\n",
        "}\n",
        "\n",
        "# ê¸°ë³¸ ëª¨ë¸\n",
        "# AUPRC ìµœì í™”ë¥¼ ìœ„í•´ eval_metric='PRAUC' ì„¤ì •\n",
        "cat_model = CatBoostClassifier(\n",
        "    loss_function='Logloss',\n",
        "    eval_metric='PRAUC',\n",
        "    verbose=0,\n",
        "    random_seed=42,\n",
        "    allow_writing_files=False\n",
        ")\n",
        "\n",
        "# RandomizedSearchCV ì„¤ì •\n",
        "search = RandomizedSearchCV(\n",
        "    cat_model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=30,                     # 30ë²ˆ ì‹œë„ (ì‹œê°„ì— ë”°ë¼ ì¡°ì ˆ ê°€ëŠ¥)\n",
        "    scoring='average_precision',   # AUPRC ê¸°ì¤€ í‰ê°€\n",
        "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "search.fit(X_tune, y_tune)\n",
        "\n",
        "best_params = search.best_params_\n",
        "print(f\"\\nâœ… ìµœì  íŒŒë¼ë¯¸í„° ë°œê²¬: {best_params}\")\n",
        "print(f\"âœ… ìµœê³  AUPRC ì ìˆ˜ (CV): {search.best_score_:.4f}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Balanced Cascade í•™ìŠµ (íŠœë‹ëœ íŒŒë¼ë¯¸í„° ì ìš©)\n",
        "# ---------------------------------------------------------\n",
        "print(f\"\\n>>> [2ë‹¨ê³„] Balanced Cascade í•™ìŠµ ì‹œì‘ (Hard Negative Mining)...\")\n",
        "\n",
        "models = []\n",
        "n_stages = 10\n",
        "current_normal_pool = X_normal_all.copy()\n",
        "\n",
        "for i in range(n_stages):\n",
        "    # 1) ì •ìƒ ë°ì´í„° ì–¸ë”ìƒ˜í”Œë§\n",
        "    if len(current_normal_pool) < len(X_fraud):\n",
        "        print(f\"  [Info] ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ {i}ë‹¨ê³„ì—ì„œ ì¢…ë£Œ.\")\n",
        "        break\n",
        "\n",
        "    X_normal_sample = current_normal_pool.sample(n=len(X_fraud), random_state=42+i)\n",
        "    y_normal_sample = pd.Series([0] * len(X_normal_sample))\n",
        "\n",
        "    # 2) í•™ìŠµ ë°ì´í„° ìƒì„±\n",
        "    X_train = pd.concat([X_fraud, X_normal_sample])\n",
        "    y_train = pd.concat([y_fraud, y_normal_sample])\n",
        "\n",
        "    # 3) CatBoost ëª¨ë¸ í•™ìŠµ (ìµœì  íŒŒë¼ë¯¸í„° ì ìš©)\n",
        "    # **best_paramsë¡œ íŠœë‹ëœ ê°’ ì–¸íŒ¨í‚¹\n",
        "    model = CatBoostClassifier(\n",
        "        **best_params,\n",
        "        loss_function='Logloss',\n",
        "        eval_metric='PRAUC',\n",
        "        verbose=0,\n",
        "        random_seed=42+i,\n",
        "        allow_writing_files=False\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    models.append(model)\n",
        "\n",
        "    # 4) Hard Negative Mining\n",
        "    if i < n_stages - 1:\n",
        "        probs = model.predict_proba(current_normal_pool)[:, 1]\n",
        "\n",
        "        # í—·ê°ˆë¦¬ëŠ” ë°ì´í„°(í™•ë¥  > 0.05)ë§Œ ë‚¨ê¹€\n",
        "        threshold_filter = 0.05\n",
        "        hard_indices = np.where(probs > threshold_filter)[0]\n",
        "        current_normal_pool = current_normal_pool.iloc[hard_indices]\n",
        "\n",
        "        print(f\" - Stage {i+1} ì™„ë£Œ. ë‚¨ì€ 'ì–´ë ¤ìš´' ì •ìƒ ë°ì´í„°: {len(current_normal_pool)}\")\n",
        "\n",
        "        if len(current_normal_pool) < len(X_fraud):\n",
        "            print(\"  -> ë‚¨ì€ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ë£¨í”„ ì¢…ë£Œ.\")\n",
        "            break\n",
        "    else:\n",
        "        print(f\" - Stage {i+1} ì™„ë£Œ (Last Stage)\")\n",
        "\n",
        "print(f\">>> ì´ {len(models)}ê°œ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. ì•™ìƒë¸” ì˜ˆì¸¡ ë° í‰ê°€\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n>>> [3ë‹¨ê³„] ì•™ìƒë¸” ì˜ˆì¸¡ ë° Threshold ìµœì í™”...\")\n",
        "\n",
        "# ì „ì²´ ë°ì´í„° í™•ë¥  ê³„ì‚°\n",
        "X_full_predict = X_full_real[X_fraud.columns]\n",
        "probs_sum = np.zeros(len(X_full_predict))\n",
        "\n",
        "for model in models:\n",
        "    probs_sum += model.predict_proba(X_full_predict)[:, 1]\n",
        "\n",
        "y_prob_ensemble = probs_sum / len(models)\n",
        "\n",
        "# AUPRC ì ìˆ˜ í™•ì¸\n",
        "final_auprc = average_precision_score(y_full_real, y_prob_ensemble)\n",
        "print(f\"ğŸ† ìµœì¢… ì•™ìƒë¸” AUPRC: {final_auprc:.6f}\")\n",
        "\n",
        "# F1 Score ê¸°ì¤€ ìµœì  ì„ê³„ê°’ ì°¾ê¸°\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_full_real, y_prob_ensemble)\n",
        "\n",
        "with np.errstate(divide='ignore', invalid='ignore'):\n",
        "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
        "    f1_scores = np.nan_to_num(f1_scores)\n",
        "\n",
        "best_idx = np.argmax(f1_scores)\n",
        "best_threshold = thresholds[best_idx]\n",
        "best_f1 = f1_scores[best_idx]\n",
        "\n",
        "print(f\"ğŸ† ìµœì  ì„ê³„ê°’ (Max F1): {best_threshold:.6f}\")\n",
        "print(f\"ğŸ† ì˜ˆìƒ ìµœê³  F1 Score: {best_f1:.6f}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. Submission ìƒì„±\n",
        "# ---------------------------------------------------------\n",
        "X_test = test[X_fraud.columns]\n",
        "test_probs_sum = np.zeros(len(X_test))\n",
        "\n",
        "for model in models:\n",
        "    test_probs_sum += model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "y_prob_test = test_probs_sum / len(models)\n",
        "final_preds = (y_prob_test >= best_threshold).astype(int)\n",
        "\n",
        "submission['Class'] = final_preds\n",
        "filename = 'submission_catboost_cascade_tuned.csv'\n",
        "submission.to_csv(filename, index=False)\n",
        "print(f\">>> {filename} ìƒì„± ì™„ë£Œ.\")\n",
        "print(f\"ì˜ˆì¸¡ëœ ì‚¬ê¸° ê±°ë˜ ìˆ˜: {submission['Class'].sum()}\")\n"
      ],
      "metadata": {
        "id": "PIlts9y-U4eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import precision_recall_curve, f1_score, average_precision_score\n",
        "import random\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. ë°ì´í„° ì¤€ë¹„\n",
        "# ---------------------------------------------------------\n",
        "# X_full_real, y_full_real, sub_train_processedê°€ ë©”ëª¨ë¦¬ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "# ì‚¬ê¸° ë°ì´í„°\n",
        "X_fraud = sub_train_processed[sub_train_processed['Class'] == 1].drop('Class', axis=1)\n",
        "y_fraud = sub_train_processed[sub_train_processed['Class'] == 1]['Class']\n",
        "\n",
        "# ì •ìƒ ë°ì´í„°\n",
        "X_normal_all = X_full_real[y_full_real == 0]\n",
        "X_normal_all = X_normal_all[X_fraud.columns] # ì»¬ëŸ¼ ìˆœì„œ ë™ê¸°í™”\n",
        "\n",
        "print(f\"ì‚¬ê¸° ë°ì´í„°: {len(X_fraud)}\")\n",
        "print(f\"ì •ìƒ ë°ì´í„°: {len(X_normal_all)}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Randomized EasyEnsemble í•™ìŠµ\n",
        "# ---------------------------------------------------------\n",
        "# Cascade ëŒ€ì‹  ë” ì•ˆì •ì ì¸ EasyEnsemble êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ë˜,\n",
        "# ëª¨ë¸ë§ˆë‹¤ íŒŒë¼ë¯¸í„°ë¥¼ ëœë¤í•˜ê²Œ ë‹¤ë¥´ê²Œ ì¤ë‹ˆë‹¤.\n",
        "\n",
        "n_fraud = len(X_fraud)\n",
        "n_normal = len(X_normal_all)\n",
        "n_bags = n_normal // n_fraud\n",
        "# ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ë©´ n_bagsë¥¼ 30~50 ì •ë„ë¡œ ì œí•œí•´ë„ ì¢‹ìŠµë‹ˆë‹¤.\n",
        "# ì—¬ê¸°ì„œëŠ” ì „ì²´ ë‹¤ ì”ë‹ˆë‹¤.\n",
        "\n",
        "print(f\"\\n>>> ì´ {n_bags}ê°œì˜ ëª¨ë¸ì„ 'ê°ê¸° ë‹¤ë¥¸ íŒŒë¼ë¯¸í„°'ë¡œ í•™ìŠµí•©ë‹ˆë‹¤...\")\n",
        "\n",
        "X_normal_shuffled = X_normal_all.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "models = []\n",
        "\n",
        "for i in range(n_bags):\n",
        "    start = i * n_fraud\n",
        "    end = (i + 1) * n_fraud\n",
        "\n",
        "    # 1) ë°ì´í„° ìƒ˜í”Œë§\n",
        "    X_normal_subset = X_normal_shuffled.iloc[start:end]\n",
        "    y_normal_subset = pd.Series([0] * len(X_normal_subset))\n",
        "\n",
        "    X_train = pd.concat([X_fraud, X_normal_subset])\n",
        "    y_train = pd.concat([y_fraud, y_normal_subset])\n",
        "\n",
        "    # 2) [í•µì‹¬] íŒŒë¼ë¯¸í„° ëœë¤ ì„¤ì • (ë‹¤ì–‘ì„± í™•ë³´)\n",
        "    # ëª¨ë¸ë§ˆë‹¤ ì¡°ê¸ˆì”© ë‹¤ë¥¸ ì„±ê²©ì„ ê°–ê²Œ ë§Œë“­ë‹ˆë‹¤.\n",
        "    random_params = {\n",
        "        'iterations': random.randint(500, 1200),\n",
        "        'learning_rate': random.uniform(0.03, 0.1),\n",
        "        'depth': random.randint(4, 8),\n",
        "        'l2_leaf_reg': random.randint(1, 9),\n",
        "        'random_strength': random.uniform(0.1, 5),\n",
        "        'bagging_temperature': random.uniform(0.0, 1.0)\n",
        "    }\n",
        "\n",
        "    model = CatBoostClassifier(\n",
        "        **random_params,\n",
        "        loss_function='Logloss',\n",
        "        eval_metric='PRAUC', # í•™ìŠµ ê³¼ì •ì€ AUPRCë¥¼ ë³´ê²Œ í•¨\n",
        "        verbose=0,\n",
        "        random_seed=42+i, # ì‹œë“œë„ ë§¤ë²ˆ ë‹¤ë¥´ê²Œ\n",
        "        allow_writing_files=False\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    models.append(model)\n",
        "\n",
        "    if (i + 1) % 50 == 0:\n",
        "        print(f\"[{i + 1}/{n_bags}] ì™„ë£Œ (Depth: {random_params['depth']}, LR: {random_params['learning_rate']:.3f})\")\n",
        "\n",
        "print(\">>> ëª¨ë“  ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. ì•™ìƒë¸” ì˜ˆì¸¡ (Soft Voting)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n>>> ì „ì²´ ë°ì´í„° ì˜ˆì¸¡ ì¤‘ (ì•™ìƒë¸”)...\")\n",
        "\n",
        "X_full_predict = X_full_real[X_fraud.columns]\n",
        "probs_sum = np.zeros(len(X_full_predict))\n",
        "\n",
        "# ê° ëª¨ë¸ì˜ í™•ë¥  í•©ì‚°\n",
        "for model in models:\n",
        "    probs_sum += model.predict_proba(X_full_predict)[:, 1]\n",
        "\n",
        "# í‰ê·  í™•ë¥ \n",
        "y_prob_ensemble = probs_sum / n_bags\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. ì„ê³„ê°’ ìµœì í™” (F1 Score ê¸°ì¤€)\n",
        "# ---------------------------------------------------------\n",
        "# 0.81ì„ ë„˜ê¸° ìœ„í•´ì„  ì—¬ê¸°ê°€ ì œì¼ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_full_real, y_prob_ensemble)\n",
        "\n",
        "with np.errstate(divide='ignore', invalid='ignore'):\n",
        "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
        "    f1_scores = np.nan_to_num(f1_scores)\n",
        "\n",
        "best_idx = np.argmax(f1_scores)\n",
        "best_threshold = thresholds[best_idx]\n",
        "best_f1 = f1_scores[best_idx]\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"ğŸ† ìµœì¢… ê²°ê³¼ ğŸ†\")\n",
        "print(f\"ìµœì  ì„ê³„ê°’: {best_threshold:.6f}\")\n",
        "print(f\"ì˜ˆìƒ ìµœê³  F1 Score: {best_f1:.6f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. Submission ìƒì„±\n",
        "# ---------------------------------------------------------\n",
        "print(\">>> ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\")\n",
        "\n",
        "X_test = test[X_fraud.columns]\n",
        "test_probs_sum = np.zeros(len(X_test))\n",
        "\n",
        "for model in models:\n",
        "    test_probs_sum += model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "y_prob_test = test_probs_sum / n_bags\n",
        "\n",
        "# ìµœì  ì„ê³„ê°’ ì ìš©\n",
        "final_preds = (y_prob_test >= best_threshold).astype(int)\n",
        "\n",
        "submission['Class'] = final_preds\n",
        "filename = 'submission_randomized_ensemble.csv'\n",
        "submission.to_csv(filename, index=False)\n",
        "\n",
        "print(f\">>> {filename} ìƒì„± ì™„ë£Œ.\")\n",
        "print(f\"ì˜ˆì¸¡ëœ ì‚¬ê¸° ê±°ë˜ ìˆ˜: {submission['Class'].sum()}\")\n"
      ],
      "metadata": {
        "id": "QQEF0MwaU4aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. ì›ë³¸ ë°ì´í„° ë¡œë“œ (Clean Start)\n",
        "# ---------------------------------------------------------\n",
        "print(\">>> ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "submission = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "# ID ì»¬ëŸ¼ ì œê±°\n",
        "if 'ID' in train_df.columns: train_df.drop('ID', axis=1, inplace=True)\n",
        "if 'ID' in test_df.columns: test_df.drop('ID', axis=1, inplace=True)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Feature Engineering (Time & Amount)\n",
        "# ---------------------------------------------------------\n",
        "print(\">>> [FE 1] Time & Amount íŒŒìƒ ë³€ìˆ˜ ìƒì„±...\")\n",
        "\n",
        "def add_time_amount_features(df):\n",
        "    df = df.copy()\n",
        "    # 1. Time -> Hour (ì‹œê°„ëŒ€) ë³€í™˜\n",
        "    # ë°ì´í„°ì…‹ì˜ Timeì€ 'ì´ˆ' ë‹¨ìœ„ì…ë‹ˆë‹¤. (86400ì´ˆ = 24ì‹œê°„)\n",
        "    df['Hour'] = (df['Time'] % 86400) // 3600\n",
        "\n",
        "    # 2. Time -> Day/Night (ë‚®/ë°¤ êµ¬ë¶„, ì„ì˜ë¡œ 06~18ì‹œë¥¼ ë‚®ìœ¼ë¡œ ì„¤ì •)\n",
        "    df['Is_Day'] = df['Hour'].apply(lambda x: 1 if 6 <= x <= 18 else 0)\n",
        "\n",
        "    # 3. Amount -> Log ë³€í™˜ (ë°ì´í„° ë¶„í¬ë¥¼ ì •ê·œë¶„í¬ì— ê°€ê¹ê²Œ)\n",
        "    df['Log_Amount'] = np.log1p(df['Amount'])\n",
        "\n",
        "    return df\n",
        "\n",
        "train_df = add_time_amount_features(train_df)\n",
        "test_df = add_time_amount_features(test_df)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Feature Engineering (Isolation Forest - Stratified OOF)\n",
        "# ---------------------------------------------------------\n",
        "print(\">>> [FE 2] Isolation Forest Anomaly Score ìƒì„± (Stratified OOF)...\")\n",
        "\n",
        "def add_anomaly_score_stratified(train_df, test_df, n_folds=5, random_seed=42):\n",
        "    train = train_df.copy()\n",
        "    test = test_df.copy()\n",
        "\n",
        "    # V ì»¬ëŸ¼ë§Œ ì‚¬ìš© (Time, Amount ì œì™¸í•˜ê³  ìˆœìˆ˜ íŒ¨í„´ë§Œ ë³´ê¸° ìœ„í•¨)\n",
        "    v_cols = [c for c in train.columns if c.startswith('V')]\n",
        "\n",
        "    # íŒŒë¼ë¯¸í„°\n",
        "    iso_params = {\n",
        "        'n_estimators': 100,\n",
        "        'max_samples': 'auto',\n",
        "        'contamination': 'auto',\n",
        "        'n_jobs': -1,\n",
        "        'random_state': random_seed\n",
        "    }\n",
        "\n",
        "    # 1) Train ë°ì´í„°: OOF ë°©ì‹ìœ¼ë¡œ ìƒì„±\n",
        "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
        "    train['anomaly_score'] = 0.0\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(train, train['Class'])):\n",
        "        X_tr = train.iloc[train_idx][v_cols]\n",
        "        X_val = train.iloc[val_idx][v_cols]\n",
        "\n",
        "        # í•™ìŠµ\n",
        "        model = IsolationForest(**iso_params)\n",
        "        model.fit(X_tr)\n",
        "\n",
        "        # ì˜ˆì¸¡ (score_samplesëŠ” ìŒìˆ˜ì¼ìˆ˜ë¡ ì´ìƒì¹˜)\n",
        "        train.iloc[val_idx, train.columns.get_loc('anomaly_score')] = model.score_samples(X_val)\n",
        "\n",
        "    # 2) Test ë°ì´í„°: ì „ì²´ Trainìœ¼ë¡œ í•™ìŠµ í›„ ìƒì„±\n",
        "    model_full = IsolationForest(**iso_params)\n",
        "    model_full.fit(train[v_cols])\n",
        "    test['anomaly_score'] = model_full.score_samples(test[v_cols])\n",
        "\n",
        "    return train, test\n",
        "\n",
        "train_df, test_df = add_anomaly_score_stratified(train_df, test_df)\n",
        "\n",
        "print(f\"í”¼ì²˜ ì¶”ê°€ ì™„ë£Œ. ì»¬ëŸ¼ ëª©ë¡: {list(train_df.columns[-5:])}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. ìŠ¤ì¼€ì¼ë§ (RobustScaler)\n",
        "# ---------------------------------------------------------\n",
        "print(\">>> ìŠ¤ì¼€ì¼ë§ ì ìš© (RobustScaler)...\")\n",
        "\n",
        "# ìŠ¤ì¼€ì¼ë§ ëŒ€ìƒ: ê¸°ì¡´ Amount, Time, ê·¸ë¦¬ê³  ìƒˆë¡œ ë§Œë“  Log_Amount, anomaly_score ë“±\n",
        "# (Vì»¬ëŸ¼ì€ ì´ë¯¸ PCA ë˜ì–´ ìˆì–´ì„œ ê±´ë“œë¦¬ì§€ ì•ŠìŒ)\n",
        "scale_cols = ['Time', 'Amount', 'Log_Amount', 'anomaly_score']\n",
        "\n",
        "scaler = RobustScaler()\n",
        "train_df[scale_cols] = scaler.fit_transform(train_df[scale_cols])\n",
        "test_df[scale_cols] = scaler.transform(test_df[scale_cols])\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. ë°ì´í„° ì¤€ë¹„ (Cascadeìš©)\n",
        "# ---------------------------------------------------------\n",
        "X_full = train_df.drop('Class', axis=1)\n",
        "y_full = train_df['Class']\n",
        "\n",
        "# ì‚¬ê¸° ë°ì´í„° ì¶”ì¶œ\n",
        "X_fraud = X_full[y_full == 1]\n",
        "y_fraud = y_full[y_full == 1]\n",
        "\n",
        "# ì •ìƒ ë°ì´í„° ì¶”ì¶œ\n",
        "X_normal_all = X_full[y_full == 0]\n",
        "\n",
        "print(f\"ìµœì¢… í•™ìŠµ ë°ì´í„° ì¤€ë¹„: ì‚¬ê¸°({len(X_fraud)}), ì •ìƒ({len(X_normal_all)})\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 6. Cascade CatBoost í•™ìŠµ (ê¸°ì¡´ 0.81ì  ë¡œì§ ë³µì›)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n>>> Balanced Cascade (CatBoost + Features) í•™ìŠµ ì‹œì‘...\")\n",
        "\n",
        "models = []\n",
        "n_stages = 10\n",
        "current_normal_pool = X_normal_all.copy()\n",
        "\n",
        "for i in range(n_stages):\n",
        "    # 1) ì •ìƒ ë°ì´í„° ì–¸ë”ìƒ˜í”Œë§\n",
        "    if len(current_normal_pool) < len(X_fraud):\n",
        "        print(f\"  [Info] ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ {i}ë‹¨ê³„ ì¡°ê¸° ì¢…ë£Œ.\")\n",
        "        break\n",
        "\n",
        "    # ëœë¤ì„±ì„ ì£¼ë˜, ë§¤ë²ˆ ë‹¤ë¥´ê²Œ\n",
        "    X_normal_sample = current_normal_pool.sample(n=len(X_fraud), random_state=42+i)\n",
        "    y_normal_sample = pd.Series([0] * len(X_normal_sample))\n",
        "\n",
        "    # 2) í•™ìŠµ ë°ì´í„° ìƒì„±\n",
        "    X_train = pd.concat([X_fraud, X_normal_sample])\n",
        "    y_train = pd.concat([y_fraud, y_normal_sample])\n",
        "\n",
        "    # 3) CatBoost ëª¨ë¸ í•™ìŠµ\n",
        "    # ê³¼ì í•©ì„ í”¼í•˜ê¸° ìœ„í•´ Randomized Search ëŒ€ì‹  'ê²€ì¦ëœ' íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
        "    # depth=6 (ê¸°ë³¸ê°’)ì´ ë³´í†µ ì•ˆì •ì ì…ë‹ˆë‹¤.\n",
        "    model = CatBoostClassifier(\n",
        "        iterations=1000,\n",
        "        learning_rate=0.05,\n",
        "        depth=6,\n",
        "        eval_metric='PRAUC', # AUPRC ê¸°ì¤€ í•™ìŠµ\n",
        "        verbose=0,\n",
        "        random_seed=42+i,\n",
        "        allow_writing_files=False\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    models.append(model)\n",
        "\n",
        "    # 4) Hard Negative Mining (Hard Example ë‚¨ê¸°ê¸°)\n",
        "    if i < n_stages - 1:\n",
        "        probs = model.predict_proba(current_normal_pool)[:, 1]\n",
        "\n",
        "        # í—·ê°ˆë¦¬ëŠ”(í™•ë¥  > 0.05) ì •ìƒ ë°ì´í„°ë§Œ ë‚¨ê¹€\n",
        "        threshold_filter = 0.05\n",
        "        hard_indices = np.where(probs > threshold_filter)[0]\n",
        "        current_normal_pool = current_normal_pool.iloc[hard_indices]\n",
        "\n",
        "        print(f\" - Stage {i+1} ì™„ë£Œ. ë‚¨ì€ 'ì–´ë ¤ìš´' ì •ìƒ ë°ì´í„°: {len(current_normal_pool)}\")\n",
        "\n",
        "        if len(current_normal_pool) < len(X_fraud):\n",
        "            break\n",
        "    else:\n",
        "        print(f\" - Stage {i+1} ì™„ë£Œ (ë§ˆì§€ë§‰).\")\n",
        "\n",
        "print(f\">>> ì´ {len(models)}ê°œ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 7. ì•™ìƒë¸” ì˜ˆì¸¡ ë° Threshold ìµœì í™”\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n>>> ê²°ê³¼ ì‚°ì¶œ ë° ìµœì í™”...\")\n",
        "\n",
        "# ì „ì²´ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ (ìµœì  ì„ê³„ê°’ ì°¾ê¸°ìš©)\n",
        "probs_sum = np.zeros(len(X_full))\n",
        "for model in models:\n",
        "    probs_sum += model.predict_proba(X_full)[:, 1]\n",
        "y_prob_ensemble = probs_sum / len(models)\n",
        "\n",
        "# AUPRC ì ìˆ˜ (ì°¸ê³ ìš©)\n",
        "auprc = average_precision_score(y_full, y_prob_ensemble)\n",
        "print(f\"ğŸ† ì „ì²´ ì•™ìƒë¸” AUPRC: {auprc:.6f}\")\n",
        "\n",
        "# F1 Score ê¸°ì¤€ ìµœì  ì„ê³„ê°’ ì°¾ê¸°\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_full, y_prob_ensemble)\n",
        "with np.errstate(divide='ignore', invalid='ignore'):\n",
        "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
        "    f1_scores = np.nan_to_num(f1_scores)\n",
        "\n",
        "best_idx = np.argmax(f1_scores)\n",
        "best_threshold = thresholds[best_idx]\n",
        "best_f1 = f1_scores[best_idx]\n",
        "\n",
        "print(f\"ğŸ† ìµœì  ì„ê³„ê°’: {best_threshold:.6f}\")\n",
        "print(f\"ğŸ† ì˜ˆìƒ F1 Score: {best_f1:.6f}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 8. Submission ìƒì„±\n",
        "# ---------------------------------------------------------\n",
        "X_test = test_df # ì´ë¯¸ ì „ì²˜ë¦¬ ì™„ë£Œë¨\n",
        "test_probs_sum = np.zeros(len(X_test))\n",
        "\n",
        "for model in models:\n",
        "    test_probs_sum += model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "y_prob_test = test_probs_sum / len(models)\n",
        "final_preds = (y_prob_test >= best_threshold).astype(int)\n",
        "\n",
        "submission['Class'] = final_preds\n",
        "filename = 'submission_catboost_cascade_FE.csv'\n",
        "submission.to_csv(filename, index=False)\n",
        "\n",
        "print(f\">>> {filename} ìƒì„± ì™„ë£Œ.\")\n",
        "print(f\"ì˜ˆì¸¡ëœ ì‚¬ê¸° ê±°ë˜ ìˆ˜: {submission['Class'].sum()}\")"
      ],
      "metadata": {
        "id": "5QIbd8FdU4X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. CatBoost ë‚´ì¥ Feature Importance (í‰ê· )\n",
        "# ---------------------------------------------------------\n",
        "print(\">>> [ë¶„ì„ 1] CatBoost ë‚´ì¥ Feature Importance ê³„ì‚° ì¤‘...\")\n",
        "\n",
        "feature_names = X_full.columns\n",
        "avg_importance = np.zeros(len(feature_names))\n",
        "\n",
        "# ëª¨ë“  Cascade ëª¨ë¸ë“¤ì˜ ì¤‘ìš”ë„ë¥¼ í•©ì‚°í•˜ì—¬ í‰ê· \n",
        "for model in models:\n",
        "    avg_importance += model.get_feature_importance()\n",
        "\n",
        "avg_importance /= len(models)\n",
        "\n",
        "# ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
        "df_imp = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': avg_importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Permutation Importance (ìˆœì—´ ì¤‘ìš”ë„)\n",
        "# ---------------------------------------------------------\n",
        "print(\">>> [ë¶„ì„ 2] Permutation Importance ê³„ì‚° ì¤‘ (ì‹œê°„ì´ ì¢€ ê±¸ë¦´ ìˆ˜ ìˆìŒ)...\")\n",
        "\n",
        "# ì „ì²´ ëª¨ë¸ì„ ë‹¤ í•˜ê¸°ì—” ì‹œê°„ì´ ê±¸ë¦¬ë¯€ë¡œ, ê°€ì¥ ëŒ€í‘œì ì¸ 'ì²« ë²ˆì§¸ ëª¨ë¸'ë¡œ ì¸¡ì •\n",
        "# (Cascade ëª¨ë¸ë“¤ì€ ì„±ê²©ì´ ë¹„ìŠ·í•˜ë¯€ë¡œ ê²½í–¥ì„±ì„ íŒŒì•…í•˜ëŠ” ë° ì¶©ë¶„í•¨)\n",
        "target_model = models[0]\n",
        "\n",
        "# scoring='average_precision' (AUPRC) ê¸°ì¤€ìœ¼ë¡œ ì¤‘ìš”ë„ ì¸¡ì •\n",
        "# n_repeats=5: 5ë²ˆ ì„ì–´ì„œ í‰ê·  ëƒ„\n",
        "perm_result = permutation_importance(\n",
        "    target_model,\n",
        "    X_full,\n",
        "    y_full,\n",
        "    scoring='average_precision',\n",
        "    n_repeats=5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "df_perm = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': perm_result.importances_mean\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. ì‹œê°í™” (ë‘ ê·¸ë˜í”„ ë¹„êµ)\n",
        "# ---------------------------------------------------------\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "# ì™¼ìª½: ë‚´ì¥ ì¤‘ìš”ë„\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.barplot(x='importance', y='feature', data=df_imp.head(20), palette='viridis')\n",
        "plt.title('CatBoost Built-in Feature Importance (Top 20)\\n(What the model learned)', fontsize=15)\n",
        "plt.xlabel('Importance Score')\n",
        "\n",
        "# ì˜¤ë¥¸ìª½: ìˆœì—´ ì¤‘ìš”ë„\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.barplot(x='importance', y='feature', data=df_perm.head(20), palette='magma')\n",
        "plt.title('Permutation Importance (Top 20)\\n(What actually matters for AUPRC)', fontsize=15)\n",
        "plt.xlabel('Decrease in AUPRC Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. í…ìŠ¤íŠ¸ë¡œ í•˜ìœ„ê¶Œ í”¼ì²˜ í™•ì¸ (ì‚­ì œ í›„ë³´)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n>>> [í•˜ìœ„ê¶Œ í”¼ì²˜ í™•ì¸ - ì‚­ì œ ê³ ë ¤] <<<\")\n",
        "print(df_perm.tail(10))\n",
        "\n",
        "print(\"\\n>>> [ìƒˆë¡œ ì¶”ê°€í•œ í”¼ì²˜ë“¤ì˜ ìˆœìœ„] <<<\")\n",
        "new_features = ['anomaly_score', 'Hour', 'Is_Day', 'Log_Amount', 'scaled_time', 'scaled_amount']\n",
        "# ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§\n",
        "existing_new_features = [f for f in new_features if f in df_perm['feature'].values]\n",
        "print(df_perm[df_perm['feature'].isin(existing_new_features)])"
      ],
      "metadata": {
        "id": "Lhu_1fp2U4Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. ë°ì´í„° ë¡œë“œ ë° ì´ˆê¸°í™”\n",
        "# ---------------------------------------------------------\n",
        "print(\">>> ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "submission = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "if 'id' in train_df.columns: train_df.drop('id', axis=1, inplace=True)\n",
        "if 'id' in test_df.columns: test_df.drop('id', axis=1, inplace=True)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. [ê°œì„ ëœ] Feature Engineering\n",
        "# ---------------------------------------------------------\n",
        "# anomaly_scoreëŠ” ì œê±°í•˜ê³ , ë„ì›€ì´ ë˜ì—ˆë˜ Log_Amountì™€ Hourë§Œ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
        "print(\">>> [FE] ìœ íš¨í•œ íŒŒìƒ ë³€ìˆ˜ ìƒì„± (Hour, Log_Amount)...\")\n",
        "\n",
        "def add_beneficial_features(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # 1. Time -> Hour (ì‹œê°„ëŒ€) : ìˆœì—´ ì¤‘ìš”ë„ ì–‘ìˆ˜(+)\n",
        "    df['Hour'] = (df['Time'] % 86400) // 3600\n",
        "\n",
        "    # 2. Time -> Day/Night (ë‚®/ë°¤) : ìˆœì—´ ì¤‘ìš”ë„ ì–‘ìˆ˜(+)\n",
        "    df['Is_Day'] = df['Hour'].apply(lambda x: 1 if 6 <= x <= 18 else 0)\n",
        "\n",
        "    # 3. Amount -> Log ë³€í™˜ : ìˆœì—´ ì¤‘ìš”ë„ ìµœìƒìœ„ ì–‘ìˆ˜(+)\n",
        "    df['Log_Amount'] = np.log1p(df['Amount'])\n",
        "\n",
        "    # [ì¤‘ìš”] Anomaly ScoreëŠ” ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤! (ë…¸ì´ì¦ˆ ì œê±°)\n",
        "\n",
        "    return df\n",
        "\n",
        "train_df = add_beneficial_features(train_df)\n",
        "test_df = add_beneficial_features(test_df)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. ìŠ¤ì¼€ì¼ë§ (RobustScaler)\n",
        "# ---------------------------------------------------------\n",
        "print(\">>> ìŠ¤ì¼€ì¼ë§ ì ìš©...\")\n",
        "# Vì»¬ëŸ¼ ì œì™¸í•˜ê³  ìŠ¤ì¼€ì¼ë§\n",
        "scale_cols = ['Time', 'Amount', 'Log_Amount', 'Hour']\n",
        "\n",
        "scaler = RobustScaler()\n",
        "train_df[scale_cols] = scaler.fit_transform(train_df[scale_cols])\n",
        "test_df[scale_cols] = scaler.transform(test_df[scale_cols])\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. ë°ì´í„°ì…‹ ì¤€ë¹„\n",
        "# ---------------------------------------------------------\n",
        "X_full = train_df.drop('Class', axis=1)\n",
        "y_full = train_df['Class']\n",
        "\n",
        "X_fraud = X_full[y_full == 1]\n",
        "y_fraud = y_full[y_full == 1]\n",
        "\n",
        "# ì •ìƒ ë°ì´í„°\n",
        "X_normal_all = X_full[y_full == 0]\n",
        "\n",
        "print(f\"í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ: ì‚¬ê¸°({len(X_fraud)}), ì •ìƒ({len(X_normal_all)})\")\n",
        "print(f\"ì‚¬ìš©ë˜ëŠ” í”¼ì²˜ ìˆ˜: {X_full.shape[1]}ê°œ\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. Cascade CatBoost í•™ìŠµ (ê²€ì¦ëœ êµ¬ì¡°)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n>>> Balanced Cascade (CatBoost) í•™ìŠµ ì‹œì‘...\")\n",
        "\n",
        "models = []\n",
        "n_stages = 10\n",
        "current_normal_pool = X_normal_all.copy()\n",
        "\n",
        "for i in range(n_stages):\n",
        "    if len(current_normal_pool) < len(X_fraud):\n",
        "        break\n",
        "\n",
        "    # ë°ì´í„° ìƒ˜í”Œë§ (ëœë¤ì„± ë¶€ì—¬)\n",
        "    X_normal_sample = current_normal_pool.sample(n=len(X_fraud), random_state=42+i)\n",
        "    y_normal_sample = pd.Series([0] * len(X_normal_sample))\n",
        "\n",
        "    X_train = pd.concat([X_fraud, X_normal_sample])\n",
        "    y_train = pd.concat([y_fraud, y_normal_sample])\n",
        "\n",
        "    # CatBoost ëª¨ë¸ (0.81ì  ë‚˜ì™”ë˜ ê¸°ë³¸ ì„¤ì • + PRAUC ìµœì í™”)\n",
        "    # ë³µì¡í•œ íŠœë‹ë³´ë‹¤ëŠ” ê¸°ë³¸ ì„±ëŠ¥ + ì¢‹ì€ í”¼ì²˜ê°€ ë‹µì…ë‹ˆë‹¤.\n",
        "    model = CatBoostClassifier(\n",
        "        iterations=1000,\n",
        "        learning_rate=0.05,\n",
        "        depth=6,\n",
        "        eval_metric='PRAUC',\n",
        "        verbose=0,\n",
        "        random_seed=42+i,\n",
        "        allow_writing_files=False\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    models.append(model)\n",
        "\n",
        "    # Hard Negative Mining\n",
        "    if i < n_stages - 1:\n",
        "        probs = model.predict_proba(current_normal_pool)[:, 1]\n",
        "\n",
        "        # í—·ê°ˆë¦¬ëŠ” ë°ì´í„°ë§Œ ë‚¨ê¸°ê¸° (Threshold 0.05)\n",
        "        hard_indices = np.where(probs > 0.05)[0]\n",
        "        current_normal_pool = current_normal_pool.iloc[hard_indices]\n",
        "\n",
        "        print(f\" - Stage {i+1} ì™„ë£Œ. ë‚¨ì€ 'ì–´ë ¤ìš´' ì •ìƒ ë°ì´í„°: {len(current_normal_pool)}\")\n",
        "\n",
        "        if len(current_normal_pool) < len(X_fraud):\n",
        "            break\n",
        "\n",
        "print(f\">>> ì´ {len(models)}ê°œ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 6. ì•™ìƒë¸” ì˜ˆì¸¡ ë° ìµœì  ì„ê³„ê°’ ì°¾ê¸°\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n>>> ê²°ê³¼ ì‚°ì¶œ ë° ìµœì í™”...\")\n",
        "\n",
        "# ì „ì²´ í•™ìŠµ ë°ì´í„° ì˜ˆì¸¡ (Threshold íŠœë‹ìš©)\n",
        "probs_sum = np.zeros(len(X_full))\n",
        "for model in models:\n",
        "    probs_sum += model.predict_proba(X_full)[:, 1]\n",
        "y_prob_ensemble = probs_sum / len(models)\n",
        "\n",
        "# AUPRC ì ìˆ˜ ì¶œë ¥\n",
        "auprc = average_precision_score(y_full, y_prob_ensemble)\n",
        "print(f\"ğŸ† ì „ì²´ ì•™ìƒë¸” AUPRC: {auprc:.6f}\")\n",
        "\n",
        "# F1 Score ê¸°ì¤€ ìµœì  ì„ê³„ê°’\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_full, y_prob_ensemble)\n",
        "with np.errstate(divide='ignore', invalid='ignore'):\n",
        "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
        "    f1_scores = np.nan_to_num(f1_scores)\n",
        "\n",
        "best_idx = np.argmax(f1_scores)\n",
        "best_threshold = thresholds[best_idx]\n",
        "best_f1 = f1_scores[best_idx]\n",
        "\n",
        "print(f\"ğŸ† ìµœì  ì„ê³„ê°’: {best_threshold:.6f}\")\n",
        "print(f\"ğŸ† ì˜ˆìƒ F1 Score: {best_f1:.6f}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 7. Submission ìƒì„±\n",
        "# ---------------------------------------------------------\n",
        "X_test = test_df # ì „ì²˜ë¦¬ ì™„ë£Œëœ ìƒíƒœ\n",
        "\n",
        "test_probs_sum = np.zeros(len(X_test))\n",
        "for model in models:\n",
        "    test_probs_sum += model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "y_prob_test = test_probs_sum / len(models)\n",
        "final_preds = (y_prob_test >= best_threshold).astype(int)\n",
        "\n",
        "submission['Class'] = final_preds\n",
        "filename = 'submission_catboost_cascade_cleaned_FE.csv'\n",
        "submission.to_csv(filename, index=False)\n",
        "\n",
        "print(f\">>> {filename} ìƒì„± ì™„ë£Œ.\")\n",
        "print(f\"ì˜ˆì¸¡ëœ ì‚¬ê¸° ê±°ë˜ ìˆ˜: {submission['Class'].sum()}\")\n"
      ],
      "metadata": {
        "id": "WedWMrjJU4Tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. CatBoost ë‚´ì¥ Feature Importance (í‰ê· )\n",
        "# ---------------------------------------------------------\n",
        "print(\">>> [ë¶„ì„ 1] CatBoost ë‚´ì¥ Feature Importance ê³„ì‚° ì¤‘...\")\n",
        "\n",
        "feature_names = X_full.columns\n",
        "avg_importance = np.zeros(len(feature_names))\n",
        "\n",
        "# ëª¨ë“  Cascade ëª¨ë¸ë“¤ì˜ ì¤‘ìš”ë„ë¥¼ í•©ì‚°í•˜ì—¬ í‰ê· \n",
        "for model in models:\n",
        "    avg_importance += model.get_feature_importance()\n",
        "\n",
        "avg_importance /= len(models)\n",
        "\n",
        "# ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
        "df_imp = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': avg_importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Permutation Importance (ìˆœì—´ ì¤‘ìš”ë„)\n",
        "# ---------------------------------------------------------\n",
        "print(\">>> [ë¶„ì„ 2] Permutation Importance ê³„ì‚° ì¤‘ (ì‹œê°„ì´ ì¢€ ê±¸ë¦´ ìˆ˜ ìˆìŒ)...\")\n",
        "\n",
        "# ì „ì²´ ëª¨ë¸ì„ ë‹¤ í•˜ê¸°ì—” ì‹œê°„ì´ ê±¸ë¦¬ë¯€ë¡œ, ê°€ì¥ ëŒ€í‘œì ì¸ 'ì²« ë²ˆì§¸ ëª¨ë¸'ë¡œ ì¸¡ì •\n",
        "# (Cascade ëª¨ë¸ë“¤ì€ ì„±ê²©ì´ ë¹„ìŠ·í•˜ë¯€ë¡œ ê²½í–¥ì„±ì„ íŒŒì•…í•˜ëŠ” ë° ì¶©ë¶„í•¨)\n",
        "target_model = models[0]\n",
        "\n",
        "# scoring='average_precision' (AUPRC) ê¸°ì¤€ìœ¼ë¡œ ì¤‘ìš”ë„ ì¸¡ì •\n",
        "# n_repeats=5: 5ë²ˆ ì„ì–´ì„œ í‰ê·  ëƒ„\n",
        "perm_result = permutation_importance(\n",
        "    target_model,\n",
        "    X_full,\n",
        "    y_full,\n",
        "    scoring='average_precision',\n",
        "    n_repeats=5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "df_perm = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': perm_result.importances_mean\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. ì‹œê°í™” (ë‘ ê·¸ë˜í”„ ë¹„êµ)\n",
        "# ---------------------------------------------------------\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "# ì™¼ìª½: ë‚´ì¥ ì¤‘ìš”ë„\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.barplot(x='importance', y='feature', data=df_imp.head(20), palette='viridis')\n",
        "plt.title('CatBoost Built-in Feature Importance (Top 20)\\n(What the model learned)', fontsize=15)\n",
        "plt.xlabel('Importance Score')\n",
        "\n",
        "# ì˜¤ë¥¸ìª½: ìˆœì—´ ì¤‘ìš”ë„\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.barplot(x='importance', y='feature', data=df_perm.head(20), palette='magma')\n",
        "plt.title('Permutation Importance (Top 20)\\n(What actually matters for AUPRC)', fontsize=15)\n",
        "plt.xlabel('Decrease in AUPRC Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. í…ìŠ¤íŠ¸ë¡œ í•˜ìœ„ê¶Œ í”¼ì²˜ í™•ì¸ (ì‚­ì œ í›„ë³´)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n>>> [í•˜ìœ„ê¶Œ í”¼ì²˜ í™•ì¸ - ì‚­ì œ ê³ ë ¤] <<<\")\n",
        "print(df_perm.tail(10))\n",
        "\n",
        "print(\"\\n>>> [ìƒˆë¡œ ì¶”ê°€í•œ í”¼ì²˜ë“¤ì˜ ìˆœìœ„] <<<\")\n",
        "new_features = ['anomaly_score', 'Hour', 'Is_Day', 'Log_Amount', 'scaled_time', 'scaled_amount']\n",
        "# ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§\n",
        "existing_new_features = [f for f in new_features if f in df_perm['feature'].values]\n",
        "print(df_perm[df_perm['feature'].isin(existing_new_features)])"
      ],
      "metadata": {
        "id": "CbcwzEYIU4RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.metrics import precision_recall_curve, f1_score, classification_report\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. ë°ì´í„° ì¤€ë¹„\n",
        "# ---------------------------------------------------------\n",
        "# 1) í•™ìŠµìš©: ì •ì œëœ ì‚¬ê¸° ë°ì´í„° 351ê°œ\n",
        "fraud_train = sub_train_processed[sub_train_processed['Class'] == 1].drop('Class', axis=1)\n",
        "\n",
        "# 2) íŠœë‹/ê²€ì¦ìš©: ì „ì²´ ì›ë³¸ ë°ì´í„° (ì •ìƒ + ì‚¬ê¸° ëª¨ë‘ í¬í•¨)\n",
        "# X_full_real, y_full_real ë³€ìˆ˜ê°€ ë©”ëª¨ë¦¬ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
        "# (Time, Amount ìŠ¤ì¼€ì¼ë§ì´ ë˜ì–´ìˆëŠ” ìƒíƒœì—¬ì•¼ í•¨)\n",
        "X_val = X_full_real[fraud_train.columns] # ì»¬ëŸ¼ ìˆœì„œ ë§ì¶”ê¸°\n",
        "y_val = y_full_real\n",
        "\n",
        "# 3) í…ŒìŠ¤íŠ¸ìš©\n",
        "X_test = test[fraud_train.columns]\n",
        "\n",
        "print(f\"í•™ìŠµ ë°ì´í„°(ì‚¬ê¸° Only) í¬ê¸°: {fraud_train.shape}\")\n",
        "print(f\"ê²€ì¦ ë°ì´í„°(ì „ì²´) í¬ê¸°: {X_val.shape}\")\n",
        "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {X_test.shape}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. One-Class SVM í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
        "# ---------------------------------------------------------\n",
        "# ì‚¬ê¸° ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ê°€ì¥ ì˜ ê°ì‹¸ëŠ” nuì™€ gammaë¥¼ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.\n",
        "# nu: í›ˆë ¨ ë°ì´í„° ì¤‘ 'ì´ìƒì¹˜(ì—¬ê¸°ì„  ì‚¬ê¸°ê°€ ì•„ë‹Œ ê²ƒ)'ë¡œ ê°„ì£¼í•  ë¹„ìœ¨ì˜ ìƒí•œì„ \n",
        "# gamma: ê²°ì • ê²½ê³„ì˜ ìœ ì—°í•¨ (í´ìˆ˜ë¡ ë³µì¡, ì‘ì„ìˆ˜ë¡ ë‹¨ìˆœ)\n",
        "\n",
        "nu_list = [0.01, 0.05, 0.1, 0.2]\n",
        "gamma_list = ['scale', 'auto', 0.001, 0.01, 0.1]\n",
        "\n",
        "best_f1 = 0\n",
        "best_params = {}\n",
        "best_model = None\n",
        "\n",
        "print(\"\\n>>> One-Class SVM íŠœë‹ ì‹œì‘...\")\n",
        "\n",
        "for nu in nu_list:\n",
        "    for gamma in gamma_list:\n",
        "        # ëª¨ë¸ ì •ì˜\n",
        "        # kernel='rbf': ë¹„ì„ í˜• ê²½ê³„ë¥¼ ê·¸ë¦¬ê¸° ìœ„í•´ í•„ìˆ˜\n",
        "        ocsvm = OneClassSVM(kernel='rbf', nu=nu, gamma=gamma)\n",
        "\n",
        "        # [í•µì‹¬] ì‚¬ê¸° ë°ì´í„°ë¡œë§Œ í•™ìŠµ (ì‚¬ê¸°ê¾¼ì˜ íŒ¨í„´ì„ ìµí˜)\n",
        "        ocsvm.fit(fraud_train)\n",
        "\n",
        "        # ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ì ìˆ˜ ì‚°ì¶œ (decision_function)\n",
        "        # ì–‘ìˆ˜(+)ë©´ í•™ìŠµí•œ í´ë˜ìŠ¤(ì‚¬ê¸°)ì— ê°€ê¹ê³ , ìŒìˆ˜(-)ë©´ ë©‚\n",
        "        scores = ocsvm.decision_function(X_val)\n",
        "\n",
        "        # F1 Score ìµœì í™”\n",
        "        precisions, recalls, thresholds = precision_recall_curve(y_val, scores)\n",
        "\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
        "            f1_scores = np.nan_to_num(f1_scores)\n",
        "\n",
        "        max_f1 = np.max(f1_scores)\n",
        "\n",
        "        if max_f1 > best_f1:\n",
        "            best_f1 = max_f1\n",
        "            best_params = {'nu': nu, 'gamma': gamma}\n",
        "            best_model = ocsvm\n",
        "            best_scores = scores\n",
        "            best_thresholds = thresholds\n",
        "            best_f1_scores = f1_scores\n",
        "\n",
        "            print(f\"New Best! F1: {best_f1:.4f} (nu={nu}, gamma={gamma})\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"ğŸ† ìµœì¢… ìµœì  íŒŒë¼ë¯¸í„°: {best_params}\")\n",
        "print(f\"ğŸ† ìµœê³  ê²€ì¦ F1 Score: {best_f1:.4f}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. ìµœì  ì„ê³„ê°’ ê²°ì •\n",
        "# ---------------------------------------------------------\n",
        "# best_modelì˜ ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìµœì  cut-off ì°¾ê¸°\n",
        "idx = np.argmax(best_f1_scores)\n",
        "best_threshold = best_thresholds[idx]\n",
        "\n",
        "print(f\"ğŸ† ìµœì  ì„ê³„ê°’(Decision Function): {best_threshold:.4f}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. Submission ìƒì„±\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n>>> Test ë°ì´í„° ì˜ˆì¸¡ ë° ì €ì¥...\")\n",
        "\n",
        "# Test ë°ì´í„°ì— ëŒ€í•œ ì ìˆ˜(ê±°ë¦¬) ê³„ì‚°\n",
        "test_scores = best_model.decision_function(X_test)\n",
        "\n",
        "# ì„ê³„ê°’ë³´ë‹¤ ì ìˆ˜ê°€ ë†’ìœ¼ë©´ 'ì‚¬ê¸°(1)', ë‚®ìœ¼ë©´ 'ì •ìƒ(0)'\n",
        "# One-Class SVMì€ í•™ìŠµí•œ í´ë˜ìŠ¤(ì‚¬ê¸°)ì™€ ìœ ì‚¬í• ìˆ˜ë¡ ì ìˆ˜ê°€ ë†’ìŠµë‹ˆë‹¤.\n",
        "final_preds = (test_scores >= best_threshold).astype(int)\n",
        "\n",
        "submission['Class'] = final_preds\n",
        "submission.to_csv('submission_ocsvm.csv', index=False)\n",
        "\n",
        "print(\">>> submission_ocsvm.csv ìƒì„± ì™„ë£Œ.\")\n",
        "print(f\"ì˜ˆì¸¡ëœ ì‚¬ê¸° ê±°ë˜ ìˆ˜: {submission['Class'].sum()}\")\n",
        "\n",
        "# ë¶„í¬ í™•ì¸ìš© ì‹œê°í™” (ì„ íƒì‚¬í•­)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(best_scores[y_val==0], label='Normal', color='blue', alpha=0.5, stat='density')\n",
        "sns.histplot(best_scores[y_val==1], label='Fraud', color='red', alpha=0.5, stat='density')\n",
        "plt.axvline(best_threshold, color='green', linestyle='--', label='Threshold')\n",
        "plt.title('One-Class SVM Decision Function Distribution')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vYhPqrPpU4O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_recall_curve, f1_score, average_precision_score\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. ë°ì´í„° ì¤€ë¹„\n",
        "# ---------------------------------------------------------\n",
        "# 1) ì‚¬ê¸° ë°ì´í„°: ì‚¬ìš©ìê°€ ì§€ì •í•œ 'ì •ì œëœ ì‚¬ê¸° ë°ì´í„°' (351ê°œ)\n",
        "fraud_data = sub_train_processed[sub_train_processed['Class'] == 1]\n",
        "X_fraud = fraud_data.drop('Class', axis=1)\n",
        "y_fraud = fraud_data['Class']\n",
        "\n",
        "# 2) ì •ìƒ ë°ì´í„°: ì „ì²´ ì›ë³¸ ë°ì´í„°ì—ì„œ ê°€ì ¸ì˜´ (ë¹„ë³µì› ì¶”ì¶œìš©)\n",
        "# (ì´ì „ì— ìƒì„±í•œ X_full_real, y_full_real ë³€ìˆ˜ ì‚¬ìš©)\n",
        "X_normal_all = X_full_real[y_full_real == 0]\n",
        "y_normal_all = y_full_real[y_full_real == 0]\n",
        "\n",
        "# [ì¤‘ìš”] ì»¬ëŸ¼ ìˆœì„œ ë™ê¸°í™”\n",
        "X_normal_all = X_normal_all[X_fraud.columns]\n",
        "\n",
        "# 3) ëª¨ë¸ ê°œìˆ˜ ê³„ì‚° (Bagging ìˆ˜)\n",
        "n_fraud = len(X_fraud)\n",
        "n_normal = len(X_normal_all)\n",
        "n_bags = n_normal // n_fraud\n",
        "\n",
        "print(f\"ì‚¬ê¸° ë°ì´í„° ìˆ˜: {n_fraud}\")\n",
        "print(f\"ì •ìƒ ë°ì´í„° ìˆ˜: {n_normal}\")\n",
        "print(f\">>> ì´ {n_bags}ê°œì˜ AdaBoost ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. EasyEnsemble í•™ìŠµ (Base: AdaBoost)\n",
        "# ---------------------------------------------------------\n",
        "# ì •ìƒ ë°ì´í„° ì„ê¸°\n",
        "X_normal_shuffled = X_normal_all.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "models = []\n",
        "\n",
        "print(\"\\n>>> í•™ìŠµ ì‹œì‘ (AdaBoost)...\")\n",
        "\n",
        "for i in range(n_bags):\n",
        "    start = i * n_fraud\n",
        "    end = (i + 1) * n_fraud\n",
        "\n",
        "    # 1) ì •ìƒ ë°ì´í„° ìŠ¬ë¼ì´ì‹±\n",
        "    X_normal_subset = X_normal_shuffled.iloc[start:end]\n",
        "    y_normal_subset = pd.Series([0] * len(X_normal_subset))\n",
        "\n",
        "    # 2) í•™ìŠµ ë°ì´í„°ì…‹ ê²°í•©\n",
        "    X_train_bag = pd.concat([X_fraud, X_normal_subset])\n",
        "    y_train_bag = pd.concat([y_fraud, y_normal_subset])\n",
        "\n",
        "    # 3) ëª¨ë¸ ìƒì„± ë° í•™ìŠµ\n",
        "    # AdaBoostì˜ ê¸°ë³¸ í•™ìŠµê¸°ëŠ” DecisionTree(max_depth=1)ì…ë‹ˆë‹¤.\n",
        "    # ì‚¬ê¸° íŒ¨í„´ì´ ë³µì¡í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ max_depth=2 ì •ë„ë¡œ ì‚´ì§ ë†’ì—¬ì¤ë‹ˆë‹¤.\n",
        "    base_estimator = DecisionTreeClassifier(max_depth=2, random_state=42+i)\n",
        "\n",
        "    model = AdaBoostClassifier(\n",
        "        estimator=base_estimator,\n",
        "        n_estimators=50,       # í•˜ë‚˜ì˜ Bag ì•ˆì—ì„œ 50ë²ˆ ë¶€ìŠ¤íŒ…\n",
        "        learning_rate=0.5,     # í•™ìŠµë¥ \n",
        "        random_state=42+i\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_bag, y_train_bag)\n",
        "    models.append(model)\n",
        "\n",
        "    if (i + 1) % 50 == 0:\n",
        "        print(f\"[{i + 1}/{n_bags}] ëª¨ë¸ í•™ìŠµ ì™„ë£Œ...\")\n",
        "\n",
        "print(\">>> ëª¨ë“  ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. ì•™ìƒë¸” ì˜ˆì¸¡ (Soft Voting)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n>>> ì „ì²´ ë°ì´í„° ì˜ˆì¸¡ ë° ìµœì  ì„ê³„ê°’ íƒìƒ‰...\")\n",
        "\n",
        "# ì „ì²´ ì›ë³¸ ë°ì´í„°(X_full_real)ì— ëŒ€í•´ ì˜ˆì¸¡ (Threshold íŠœë‹ìš©)\n",
        "# ì»¬ëŸ¼ ìˆœì„œ ë§ì¶”ê¸°\n",
        "X_full_predict = X_full_real[X_fraud.columns]\n",
        "probs_sum = np.zeros(len(X_full_predict))\n",
        "\n",
        "# ëª¨ë“  ëª¨ë¸ì˜ í™•ë¥  í•©ì‚°\n",
        "for model in models:\n",
        "    probs_sum += model.predict_proba(X_full_predict)[:, 1]\n",
        "\n",
        "# í‰ê·  í™•ë¥ \n",
        "y_prob_ensemble = probs_sum / n_bags\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. ì„ê³„ê°’ ìµœì í™” (F1 Score ê¸°ì¤€)\n",
        "# ---------------------------------------------------------\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_full_real, y_prob_ensemble)\n",
        "\n",
        "with np.errstate(divide='ignore', invalid='ignore'):\n",
        "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
        "    f1_scores = np.nan_to_num(f1_scores)\n",
        "\n",
        "best_idx = np.argmax(f1_scores)\n",
        "best_threshold = thresholds[best_idx]\n",
        "best_f1 = f1_scores[best_idx]\n",
        "\n",
        "# AUPRC ì ìˆ˜ í™•ì¸\n",
        "final_auprc = average_precision_score(y_full_real, y_prob_ensemble)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"ğŸ† AdaBoost EasyEnsemble ê²°ê³¼ ğŸ†\")\n",
        "print(f\"AUPRC Score: {final_auprc:.6f}\")\n",
        "print(f\"ìµœì  ì„ê³„ê°’: {best_threshold:.6f}\")\n",
        "print(f\"ì˜ˆìƒ ìµœê³  F1 Score: {best_f1:.6f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. Submission ìƒì„±\n",
        "# ---------------------------------------------------------\n",
        "print(\">>> ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\")\n",
        "\n",
        "X_test = test[X_fraud.columns]\n",
        "test_probs_sum = np.zeros(len(X_test))\n",
        "\n",
        "for model in models:\n",
        "    test_probs_sum += model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "y_prob_test_ensemble = test_probs_sum / n_bags\n",
        "\n",
        "# ìµœì  ì„ê³„ê°’ ì ìš©\n",
        "final_preds = (y_prob_test_ensemble >= best_threshold).astype(int)\n",
        "\n",
        "submission['Class'] = final_preds\n",
        "filename = 'submission_adaboost_ensemble.csv'\n",
        "submission.to_csv(filename, index=False)\n",
        "\n",
        "print(f\">>> {filename} ìƒì„± ì™„ë£Œ.\")\n",
        "print(f\"ì˜ˆì¸¡ëœ ì‚¬ê¸° ê±°ë˜ ìˆ˜: {submission['Class'].sum()}\")\n"
      ],
      "metadata": {
        "id": "pXThJ1UrU4Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from sklearn.metrics import average_precision_score, precision_recall_curve, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
        "\n",
        "# 1. ë°ì´í„° ë¡œë“œ\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "submission = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "# 2. ì „ì²˜ë¦¬: ID ì»¬ëŸ¼ ì œê±° ë° X, y ë¶„ë¦¬\n",
        "if 'id' in train.columns:\n",
        "    X = train.drop(['id', 'Class'], axis=1)\n",
        "else:\n",
        "    X = train.drop('Class', axis=1)\n",
        "y = train['Class']\n",
        "\n",
        "if 'id' in test.columns:\n",
        "    X_test = test.drop('id', axis=1)\n",
        "else:\n",
        "    X_test = test\n",
        "\n",
        "# 3. ëª¨ë¸ ì •ì˜: Balanced Random Forest (imblearn ì‚¬ìš©)\n",
        "# sampling_strategy='auto'ëŠ” ìë™ìœ¼ë¡œ 1:1 ë¹„ìœ¨(majority classë¥¼ minority class ê°œìˆ˜ë§Œí¼ ì–¸ë”ìƒ˜í”Œë§)ë¡œ ë§ì¶¥ë‹ˆë‹¤.\n",
        "brf = BalancedRandomForestClassifier(\n",
        "    n_estimators=300,          # ì¶©ë¶„í•œ íŠ¸ë¦¬ ê°œìˆ˜\n",
        "    sampling_strategy='auto',  # í•µì‹¬: ìë™ìœ¼ë¡œ 1:1 ë¹„ìœ¨ë¡œ ì–¸ë”ìƒ˜í”Œë§ ìˆ˜í–‰\n",
        "    replacement=False,         # ë¹„ë³µì› ì¶”ì¶œ (ì¼ë°˜ì ìœ¼ë¡œ ë” ì¢‹ìŒ)\n",
        "    random_state=42,\n",
        "    n_jobs=-1                  # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©\n",
        ")\n",
        "\n",
        "# 4. êµì°¨ ê²€ì¦(CV)ì„ í†µí•œ ìµœì  ì„ê³„ê°’ ì°¾ê¸° (Data Leakage ë°©ì§€)\n",
        "# í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ Out-of-Fold ì˜ˆì¸¡ í™•ë¥  ìƒì„±\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "y_oof_proba = cross_val_predict(brf, X, y, cv=kf, method='predict_proba')[:, 1]\n",
        "\n",
        "# 5. ì„±ëŠ¥ í‰ê°€ ë° ì„ê³„ê°’ ìµœì í™”\n",
        "# AUPRC ê³„ì‚°\n",
        "auprc = average_precision_score(y, y_oof_proba)\n",
        "print(f\"Cross-Validated AUPRC: {auprc:.4f}\")\n",
        "\n",
        "# Precision-Recall Curveë¥¼ í†µí•´ F1 ì ìˆ˜ê°€ ìµœëŒ€ê°€ ë˜ëŠ” ì„ê³„ê°’ íƒìƒ‰\n",
        "precisions, recalls, thresholds = precision_recall_curve(y, y_oof_proba)\n",
        "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10) # 0 ë‚˜ëˆ„ê¸° ë°©ì§€\n",
        "best_idx = np.argmax(f1_scores)\n",
        "best_threshold = thresholds[best_idx]\n",
        "best_f1 = f1_scores[best_idx]\n",
        "\n",
        "print(f\"Optimal Threshold: {best_threshold:.4f}\")\n",
        "print(f\"Maximized F1-Score (CV): {best_f1:.4f}\")\n",
        "\n",
        "# 6. ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
        "brf.fit(X, y)\n",
        "test_proba = brf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# ìµœì  ì„ê³„ê°’ ì ìš©í•˜ì—¬ 0/1 ì˜ˆì¸¡ ìƒì„±\n",
        "final_preds = (test_proba >= best_threshold).astype(int)\n",
        "\n",
        "# 7. ì œì¶œ íŒŒì¼ ìƒì„±\n",
        "submission['Class'] = final_preds\n",
        "submission.to_csv('submission_brf_implearn.csv', index=False)\n",
        "print(\"submission_brf_implearn.csv ì €ì¥ ì™„ë£Œ\")\n"
      ],
      "metadata": {
        "id": "zIuTtdIyU4Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡"
      ],
      "metadata": {
        "id": "si44Ih13U4GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install imbalanced-learn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import gc\n",
        "\n",
        "# Scikit-learn & Imblearn\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import f1_score, roc_auc_score, precision_recall_curve, classification_report, confusion_matrix\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Sampling\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Models\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==========================================\n",
        "# 1ï¸âƒ£ ë°ì´í„° ë¡œë“œ & ì „ì²˜ë¦¬\n",
        "# ==========================================\n",
        "print(\"ğŸ“¥ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì‹œì‘...\")\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "submission = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "# Train/Test ë³‘í•©\n",
        "train['is_train'] = 1\n",
        "test['is_train'] = 0\n",
        "target = train['Class']\n",
        "df_all = pd.concat([train.drop(['Class'], axis=1), test], axis=0).reset_index(drop=True)\n",
        "\n",
        "# ğŸ›  Feature Engineering\n",
        "# 1. Time ì£¼ê¸°ì„±\n",
        "df_all['hour'] = df_all['Time'] % (24 * 3600) / 3600\n",
        "df_all['hour_sin'] = np.sin(2 * np.pi * df_all['hour'] / 24)\n",
        "df_all['hour_cos'] = np.cos(2 * np.pi * df_all['hour'] / 24)\n",
        "\n",
        "# 2. Amount Log & Scaling\n",
        "df_all['Amount_log'] = np.log1p(df_all['Amount'])\n",
        "scaler = RobustScaler()\n",
        "df_all['Amount_scaled'] = scaler.fit_transform(df_all[['Amount']].values)\n",
        "\n",
        "# 3. ì¤‘ìš” ìƒí˜¸ì‘ìš© (V17, V14, V12, V10 ë“± ì‚¬ê¸° íŒ¨í„´ í•µì‹¬)\n",
        "df_all['V17_V14'] = df_all['V17'] * df_all['V14']\n",
        "df_all['V12_V10'] = df_all['V12'] * df_all['V10']\n",
        "df_all['V14_V12'] = df_all['V14'] * df_all['V12']\n",
        "\n",
        "# 4. ë¶ˆí•„ìš” ì»¬ëŸ¼ ì‚­ì œ\n",
        "df_all = df_all.drop(columns=['Time', 'Amount', 'hour'])\n",
        "\n",
        "# ë°ì´í„° ë‹¤ì‹œ ë¶„ë¦¬\n",
        "X = df_all[df_all['is_train'] == 1].drop(['is_train'], axis=1)\n",
        "X_test = df_all[df_all['is_train'] == 0].drop(['is_train'], axis=1)\n",
        "y = target\n",
        "\n",
        "# ==========================================\n",
        "# 2ï¸âƒ£ ìƒ˜í”Œë§ íŒŒì´í”„ë¼ì¸ ì •ì˜ (SMOTE + Undersampling)\n",
        "# ==========================================\n",
        "# ì „ëµ:\n",
        "# 1. SMOTEë¡œ ì‚¬ê¸° ë°ì´í„°ë¥¼ ì „ì²´ì˜ 10% ìˆ˜ì¤€ê¹Œì§€ ë»¥íŠ€ê¸° (ì •ë³´ ì¦ê°•)\n",
        "# 2. RandomUnderSamplerë¡œ ì •ìƒ ë°ì´í„°ë¥¼ ì¤„ì—¬ì„œ ì‚¬ê¸°:ì •ìƒ ë¹„ìœ¨ì„ 1:2 (0.5) ì •ë„ë¡œ ë§ì¶¤\n",
        "# -> ë„ˆë¬´ 1:1ë¡œ ë§ì¶”ë©´ ì •ìƒ ë°ì´í„° ì •ë³´ ì†ì‹¤ì´ í¼. 1:2 ì •ë„ê°€ í™©ê¸ˆë¹„ìœ¨ì¼ ë•Œê°€ ë§ìŒ.\n",
        "# -> ì—¬ê¸°ì„œëŠ” ìš”ì²­ì— ë”°ë¼ ê°•ë ¥í•˜ê²Œ ëŒ€ì‘í•˜ê¸° ìœ„í•´ 1:1ì— ê°€ê¹ê²Œ ì¡°ì • ì‹œë„.\n",
        "\n",
        "# ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ë°ì´í„° íƒ€ì… ë³€í™˜\n",
        "X = X.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)\n",
        "\n",
        "# ==========================================\n",
        "# 3ï¸âƒ£ Stratified K-Fold + Sampling Loop\n",
        "# ==========================================\n",
        "print(\"ğŸ”„ CV Training with SMOTE & Undersampling...\")\n",
        "\n",
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "oof_preds = np.zeros(len(X))\n",
        "test_preds = np.zeros(len(X_test))\n",
        "\n",
        "for n_fold, (train_idx, val_idx) in enumerate(folds.split(X, y)):\n",
        "    # 1. ë°ì´í„° ë¶„í• \n",
        "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "\n",
        "    # 2. ìƒ˜í”Œë§ ì ìš© (Train ë°ì´í„°ì—ë§Œ ì ìš©! Validation ê±´ë“œë¦¬ë©´ ì•ˆë¨)\n",
        "    # sampling_strategy=0.1 : Minorityë¥¼ Majorityì˜ 10%ê¹Œì§€ ìƒì„±\n",
        "    # sampling_strategy=0.5 : Majorityë¥¼ Minorityì˜ 2ë°°(0.5ë¹„ìœ¨)ê°€ ë˜ë„ë¡ ê¹ìŒ\n",
        "    over = SMOTE(sampling_strategy=0.2, random_state=42)\n",
        "    under = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\n",
        "\n",
        "    steps = [('o', over), ('u', under)]\n",
        "    pipeline = ImbPipeline(steps=steps)\n",
        "\n",
        "    print(f\"   [Fold {n_fold+1}] Sampling ì „: {y_train.value_counts().to_dict()}\")\n",
        "    X_train_res, y_train_res = pipeline.fit_resample(X_train, y_train)\n",
        "    print(f\"   [Fold {n_fold+1}] Sampling í›„: {y_train_res.value_counts().to_dict()}\")\n",
        "\n",
        "    # 3. ëª¨ë¸ í•™ìŠµ (ë°ì´í„°ê°€ ì–´ëŠì •ë„ ê· í˜• ì¡í˜”ìœ¼ë¯€ë¡œ scale_pos_weight ì œê±°í•˜ê±°ë‚˜ ë‚®ì¶¤)\n",
        "    # XGBoost\n",
        "    model = xgb.XGBClassifier(\n",
        "        n_estimators=2000,\n",
        "        learning_rate=0.02,\n",
        "        max_depth=8,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        # scale_pos_weightëŠ” ìƒ˜í”Œë§ì„ í–ˆìœ¼ë¯€ë¡œ ì œê±° (ë˜ëŠ” 1.0)\n",
        "        tree_method='hist',\n",
        "        objective='binary:logistic',\n",
        "        eval_metric='auc',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_res, y_train_res,\n",
        "              eval_set=[(X_val, y_val)],\n",
        "              verbose=False)\n",
        "\n",
        "    # Best Iteration ê¸°ì¤€ ì˜ˆì¸¡\n",
        "    # XGBoostëŠ” fit í›„ ìë™ìœ¼ë¡œ best_iteration ì‚¬ìš©í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜\n",
        "    # ì—¬ê¸°ì„œëŠ” early_stopping ì—†ì´ ì¶©ë¶„íˆ í•™ìŠµ í›„ ì „ì²´ ê²½í–¥ ë°˜ì˜ (Validation ScoreëŠ” Checkìš©)\n",
        "\n",
        "    # 4. ì˜ˆì¸¡ (OOF & Test)\n",
        "    oof_preds[val_idx] = model.predict_proba(X_val)[:, 1]\n",
        "    test_preds += model.predict_proba(X_test)[:, 1] / folds.n_splits\n",
        "\n",
        "# ==========================================\n",
        "# 4ï¸âƒ£ Threshold ìµœì í™” (F1 Maximization)\n",
        "# ==========================================\n",
        "print(\"\\nğŸ¯ ìµœì  Threshold íƒìƒ‰ ì¤‘...\")\n",
        "\n",
        "precisions, recalls, thresholds = precision_recall_curve(y, oof_preds)\n",
        "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
        "\n",
        "best_idx = np.argmax(f1_scores)\n",
        "best_thresh = thresholds[best_idx]\n",
        "best_f1 = f1_scores[best_idx]\n",
        "\n",
        "print(f\"âœ… Best Threshold : {best_thresh:.6f}\")\n",
        "print(f\"âœ… Max Valid F1   : {best_f1:.6f}\")\n",
        "print(f\"âœ… Valid ROC-AUC  : {roc_auc_score(y, oof_preds):.6f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "oof_binary = (oof_preds >= best_thresh).astype(int)\n",
        "cm = confusion_matrix(y, oof_binary)\n",
        "print(\"\\n[Confusion Matrix]\")\n",
        "print(cm)\n",
        "print(classification_report(y, oof_binary))\n",
        "\n",
        "# ==========================================\n",
        "# 5ï¸âƒ£ ì œì¶œ íŒŒì¼ ìƒì„±\n",
        "# ==========================================\n",
        "final_binary_pred = (test_preds >= best_thresh).astype(int)\n",
        "\n",
        "# sample_submission.csv í˜•ì‹ í™•ì¸\n",
        "if 'Class' in submission.columns:\n",
        "    submission['Class'] = final_binary_pred\n",
        "else:\n",
        "    submission.iloc[:, 1] = final_binary_pred\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(f\"\\nğŸ“„ submission.csv ì €ì¥ ì™„ë£Œ! (ì˜ˆì¸¡ëœ ì‚¬ê¸° ê±´ìˆ˜: {final_binary_pred.sum()}ê±´)\")\n"
      ],
      "metadata": {
        "id": "nNwPMslcU4D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WUFw7x80U4Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SfjpuN0xU3_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rrUZktnbU39O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "StMP-SO4U36v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m_X0Mr9gU34K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZRQAT5cmLN3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0UGvaAXTLN0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡"
      ],
      "metadata": {
        "id": "A-6g4F1DLNyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "submission = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "train = train.drop(columns=['id']).copy()\n",
        "test = test.drop(columns=['id']).copy()"
      ],
      "metadata": {
        "id": "_AJmO1JbLNvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IZsdSmUYLNrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 1. ë°ì´í„° ì¤€ë¹„ (ì´ë¯¸ ë¡œë“œëœ train, test ì´ìš©)\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "# íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬\n",
        "# id ì»¬ëŸ¼ì€ ì´ë¯¸ ì œê±°í•˜ì…¨ë‹¤ê³  í–ˆìœ¼ë¯€ë¡œ ë°”ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.\n",
        "X = train.drop('Class', axis=1)\n",
        "y = train['Class']\n",
        "\n",
        "# í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ë¶„ë¦¬\n",
        "# ê²€ì¦ ë°ì´í„°ê°€ ìˆì–´ì•¼ 'AUPRC'ë¥¼ ì¸¡ì •í•˜ê³  'ì„ê³„ê°’ ìµœì í™”'ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "# stratify=y ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ê¸°(1) ë°ì´í„° ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©° ë‚˜ëˆ•ë‹ˆë‹¤.\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 2. LightGBM ëª¨ë¸ í•™ìŠµ (Metric: Average Precision)\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "# ë¶ˆê· í˜• ê°€ì¤‘ì¹˜ ê³„ì‚° (ìŒì„± í´ë˜ìŠ¤ ê°œìˆ˜ / ì–‘ì„± í´ë˜ìŠ¤ ê°œìˆ˜)\n",
        "ratio = y_train.value_counts()[0] / y_train.value_counts()[1]\n",
        "print(f\"ë¶ˆê· í˜• ë¹„ìœ¨(scale_pos_weight): {ratio:.2f}\")\n",
        "\n",
        "lgbm_clf = lgb.LGBMClassifier(\n",
        "    objective='binary',\n",
        "    metric='average_precision',       # AUPRCë¥¼ ìµœì í™” ì§€í‘œë¡œ ì‚¬ìš©\n",
        "    n_estimators=2000,                # ë„‰ë„‰í•˜ê²Œ ì„¤ì • í›„ ì¡°ê¸° ì¢…ë£Œë¡œ ì œì–´\n",
        "    learning_rate=0.02,               # í•™ìŠµë¥ ì„ ì¡°ê¸ˆ ë‚®ì¶°ì„œ ê¼¼ê¼¼í•˜ê²Œ í•™ìŠµ\n",
        "    num_leaves=31,\n",
        "    max_depth=-1,\n",
        "    scale_pos_weight=ratio,           # ë¶ˆê· í˜• ë°ì´í„° ê°€ì¤‘ì¹˜ ì ìš©\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1                        # ë¶ˆí•„ìš”í•œ ê²½ê³  ë©”ì‹œì§€ ë„ê¸°\n",
        ")\n",
        "\n",
        "print(\"\\n[LightGBM í•™ìŠµ ì‹œì‘...]\")\n",
        "lgbm_clf.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=True)] # 100ë²ˆ ë™ì•ˆ ì„±ëŠ¥ í–¥ìƒ ì—†ìœ¼ë©´ ë©ˆì¶¤\n",
        ")\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 3. ì„ê³„ê°’(Threshold) ìë™ ìµœì í™” (F1-Score ê¸°ì¤€)\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "# ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ í™•ë¥  ì˜ˆì¸¡ (Class 1ì¼ í™•ë¥ )\n",
        "val_pred_proba = lgbm_clf.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# Precision-Recall Curve ê³„ì‚°\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_val, val_pred_proba)\n",
        "\n",
        "# F1 Score ê³„ì‚° (thresholds ê°œìˆ˜ì— ë§ì¶°ì„œ ê³„ì‚°)\n",
        "# ë§ˆì§€ë§‰ precision, recallì€ 1, 0ìœ¼ë¡œ thresholdì™€ ë§¤í•‘ë˜ì§€ ì•Šì•„ ìŠ¬ë¼ì´ì‹± í•„ìš”\n",
        "f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
        "f1_scores = np.nan_to_num(f1_scores) # 0 ë‚˜ëˆ„ê¸° ë°©ì§€\n",
        "\n",
        "# ìµœì ì˜ F1 Scoreë¥¼ ë§Œë“œëŠ” ì¸ë±ìŠ¤ ì°¾ê¸°\n",
        "best_idx = np.argmax(f1_scores)\n",
        "best_threshold = thresholds[best_idx]\n",
        "best_f1 = f1_scores[best_idx]\n",
        "\n",
        "print(f\"\\n[ìµœì í™” ê²°ê³¼]\")\n",
        "print(f\"Best Threshold (ì„ê³„ê°’): {best_threshold:.6f}\")\n",
        "print(f\"Best Validation F1-Score: {best_f1:.6f}\")\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 4. ê²€ì¦ ë°ì´í„° ìµœì¢… í‰ê°€ (ìµœì  ì„ê³„ê°’ ì ìš©)\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "val_pred_final = (val_pred_proba >= best_threshold).astype(int)\n",
        "\n",
        "print(\"\\n[Validation Set Report with Optimized Threshold]\")\n",
        "print(classification_report(y_val, val_pred_final))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, val_pred_final))\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 5. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° í™•ë¥  ì˜ˆì¸¡\n",
        "test_pred_proba = lgbm_clf.predict_proba(test)[:, 1]\n",
        "\n",
        "# ìµœì  ì„ê³„ê°’ ì ìš©í•˜ì—¬ 0 ë˜ëŠ” 1ë¡œ ë³€í™˜\n",
        "test_pred_final = (test_pred_proba >= best_threshold).astype(int)\n",
        "\n",
        "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
        "submission['Class'] = test_pred_final\n",
        "submission.to_csv('submission_lgbm_optimized.csv', index=False)\n",
        "\n",
        "print(f\"\\nì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: submission_lgbm_optimized.csv\")\n",
        "print(f\"ì˜ˆì¸¡ëœ ì‚¬ê¸° ê±°ë˜(1) ê°œìˆ˜: {submission['Class'].sum()}\")"
      ],
      "metadata": {
        "id": "EW-tuttOLNpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# í”¼ì²˜ ì¤‘ìš”ë„ ë° ìˆœì—´ ì¤‘ìš”ë„ ì‹œê°í™” (LGBM ì „ìš©)\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "def visualize_lgbm_importances(model, X_val, y_val, scoring_metric='average_precision'):\n",
        "    # 1. í”¼ì²˜ ì´ë¦„ ì¶”ì¶œ\n",
        "    feature_names = X_val.columns.tolist()\n",
        "\n",
        "    # 2. Built-in Feature Importance (Gain & Split) ê°€ì ¸ì˜¤ê¸°\n",
        "    # LGBMBooster ê°ì²´ì—ì„œ ì§ì ‘ ê°€ì ¸ì™€ì•¼ ì •í™•í•˜ê²Œ gain/split êµ¬ë¶„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
        "    booster = model.booster_\n",
        "\n",
        "    # Gain (ì •ë³´ ì´ë“: ì´ í”¼ì²˜ê°€ ë¶ˆìˆœë„ë¥¼ ì–¼ë§ˆë‚˜ ì¤„ì˜€ëŠ”ê°€?)\n",
        "    imp_gain = booster.feature_importance(importance_type='gain')\n",
        "    df_gain = pd.DataFrame({'feature': feature_names, 'importance': imp_gain})\n",
        "    df_gain = df_gain.sort_values(by='importance', ascending=False)\n",
        "\n",
        "    # Split (ë¶„ê¸° íšŸìˆ˜: ëª¨ë¸ì´ ì´ í”¼ì²˜ë¥¼ ëª‡ ë²ˆ ì‚¬ìš©í–ˆëŠ”ê°€?)\n",
        "    imp_split = booster.feature_importance(importance_type='split')\n",
        "    df_split = pd.DataFrame({'feature': feature_names, 'importance': imp_split})\n",
        "    df_split = df_split.sort_values(by='importance', ascending=False)\n",
        "\n",
        "    # 3. Permutation Importance (ìˆœì—´ ì¤‘ìš”ë„) ê³„ì‚°\n",
        "    # íŠ¹ì • ì»¬ëŸ¼ì„ ë¬´ì‘ìœ„ë¡œ ì„ì—ˆì„ ë•Œ ì„±ëŠ¥(ì—¬ê¸°ì„œëŠ” AUPRC)ì´ ì–¼ë§ˆë‚˜ ë–¨ì–´ì§€ëŠ”ì§€ ì¸¡ì •\n",
        "    print(f\"\\n[Permutation Importance ê³„ì‚° ì¤‘... í‰ê°€ ì§€í‘œ: {scoring_metric}]\")\n",
        "    print(\"ë°ì´í„° í¬ê¸°ì— ë”°ë¼ ì‹œê°„ì´ ë‹¤ì†Œ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤...\")\n",
        "\n",
        "    perm_result = permutation_importance(\n",
        "        model, X_val, y_val,\n",
        "        n_repeats=10,           # ë°˜ë³µ íšŸìˆ˜\n",
        "        scoring=scoring_metric, # ì¤‘ìš”! ë¶ˆê· í˜• ë°ì´í„°ì´ë¯€ë¡œ average_precision ì‚¬ìš©\n",
        "        n_jobs=-1,              # ë³‘ë ¬ ì²˜ë¦¬\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # ìˆœì—´ ì¤‘ìš”ë„ ê²°ê³¼ ì •ë ¬ (í‰ê· ê°’ ê¸°ì¤€)\n",
        "    perm_sorted_idx = perm_result.importances_mean.argsort()\n",
        "\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # 4. ì‹œê°í™” (3ê°œ ê·¸ë˜í”„ë¥¼ ë‚˜ë€íˆ ì¶œë ¥)\n",
        "    # --------------------------------------------------------------------------------\n",
        "    plt.figure(figsize=(24, len(feature_names) * 0.5)) # í”¼ì²˜ ìˆ˜ì— ë§ì¶° ë†’ì´ ìë™ ì¡°ì ˆ\n",
        "\n",
        "    # (1) Gain Plot\n",
        "    plt.subplot(1, 3, 1)\n",
        "    sns.barplot(x='importance', y='feature', data=df_gain, palette='viridis')\n",
        "    plt.title('Feature Importance (Gain)\\n(High Information Gain)', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Gain')\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
        "\n",
        "    # (2) Split Plot\n",
        "    plt.subplot(1, 3, 2)\n",
        "    sns.barplot(x='importance', y='feature', data=df_split, palette='viridis')\n",
        "    plt.title('Feature Importance (Split)\\n(Frequency of Use)', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Times Used')\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
        "\n",
        "    # (3) Permutation Importance Plot\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.boxplot(\n",
        "        perm_result.importances[perm_sorted_idx].T,\n",
        "        vert=False,\n",
        "        labels=np.array(feature_names)[perm_sorted_idx]\n",
        "    )\n",
        "    plt.title(f'Permutation Importance\\n(Drop in {scoring_metric})', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Importance Score (Performance Decrease)')\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# í•¨ìˆ˜ ì‹¤í–‰ (ì´ë¯¸ í•™ìŠµëœ lgbm_clf, X_val, y_val ë³€ìˆ˜ ì‚¬ìš©)\n",
        "visualize_lgbm_importances(lgbm_clf, X_val, y_val)\n"
      ],
      "metadata": {
        "id": "iO9xVIjqLNm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, confusion_matrix, classification_report\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 1. Feature Engineering í•¨ìˆ˜ (V14, V4, V12, V10 ì§‘ì¤‘ ê³µëµ)\n",
        "# --------------------------------------------------------------------------------\n",
        "def engineer_features(df_train, df_test):\n",
        "    \"\"\"\n",
        "    ì¤‘ìš”ë„ê°€ ë†’ì•˜ë˜ V14, V4, V12, V10ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìƒí˜¸ì‘ìš© ë° íŒŒìƒ ë³€ìˆ˜ ìƒì„±\n",
        "    \"\"\"\n",
        "    print(\"[Feature Engineering] íŒŒìƒ ë³€ìˆ˜ ìƒì„± ì¤‘...\")\n",
        "\n",
        "    # ì›ë³¸ ë°ì´í„° ë³´ì¡´ì„ ìœ„í•´ ë³µì‚¬\n",
        "    train_fe = df_train.copy()\n",
        "    test_fe = df_test.copy()\n",
        "\n",
        "    # ê³µí†µ ì²˜ë¦¬ë¥¼ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ìŒ\n",
        "    datasets = [train_fe, test_fe]\n",
        "\n",
        "    for df in datasets:\n",
        "        # A. ìƒí˜¸ì‘ìš© í”¼ì²˜ (Interaction Features) - ìµœìƒìœ„ í”¼ì²˜ë“¤ ê°„ì˜ ê²°í•©\n",
        "        # ì‚¬ê¸° íŒ¨í„´ ì¦í­: V14(ìŒìˆ˜ ì„±í–¥) * V4(ì–‘ìˆ˜ ì„±í–¥) ë“±\n",
        "        df['V14_V4_prod'] = df['V14'] * df['V4']\n",
        "        df['V14_V12_prod'] = df['V14'] * df['V12']\n",
        "        df['V14_V10_prod'] = df['V14'] * df['V10']\n",
        "\n",
        "        # ì°¨ì´(Difference) - íŒ¨í„´ì˜ ê²©ì°¨ ê°•ì¡°\n",
        "        df['V14_V4_diff'] = df['V14'] - df['V4']\n",
        "        df['V14_V12_diff'] = df['V14'] - df['V12']\n",
        "\n",
        "        # B. í†µê³„ì  ìš”ì•½ í”¼ì²˜ (Row-wise Statistics)\n",
        "        # ìƒìœ„ ì¤‘ìš” í”¼ì²˜ë“¤ì´ ì „ì²´ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ í”ë“¤ë¦¬ëŠ”ì§€ íŒŒì•…\n",
        "        top_feats = ['V14', 'V4', 'V12', 'V10', 'V11', 'V17']\n",
        "        df['top_feats_mean'] = df[top_feats].mean(axis=1)\n",
        "        df['top_feats_std'] = df[top_feats].std(axis=1) # ë³€ë™ì„±\n",
        "        df['top_feats_range'] = df[top_feats].max(axis=1) - df[top_feats].min(axis=1) # ë²”ìœ„\n",
        "\n",
        "        # 1ì¼ = 24ì‹œê°„ * 60ë¶„ * 60ì´ˆ = 86400ì´ˆ\n",
        "        seconds_in_day = 24 * 60 * 60\n",
        "\n",
        "        # ì£¼ê¸°ì„±ì„ ë°˜ì˜í•˜ê¸° ìœ„í•´ sin, cos ë³€í™˜\n",
        "        df['hour_sin'] = np.sin(2 * np.pi * df['Time'] / seconds_in_day)\n",
        "        df['hour_cos'] = np.cos(2 * np.pi * df['Time'] / seconds_in_day)\n",
        "\n",
        "        # ì°¸ê³ : ì›ë³¸ Time ë° hour ì»¬ëŸ¼ ë“±ì€ ìš”ì²­ëŒ€ë¡œ ì‚­ì œí•˜ì§€ ì•Šê³  ìœ ì§€í•¨\n",
        "\n",
        "        # C. Amount ë¡œê·¸ ë³€í™˜ (ì™œë„ ì™„í™”)\n",
        "        df['Amount_log'] = np.log1p(df['Amount'])\n",
        "\n",
        "    # Amount ì›ë³¸ì€ ë¡œê·¸ ë³€í™˜ëœ ê²Œ ìˆìœ¼ë¯€ë¡œ ì œê±°í•´ë„ ë¨ (ì„ íƒì‚¬í•­, ì—¬ê¸°ì„  ìœ ì§€)\n",
        "\n",
        "    print(f\"ìƒì„±ëœ íŒŒìƒ ë³€ìˆ˜ í¬í•¨ ì´ ì»¬ëŸ¼ ìˆ˜: {train_fe.shape[1]}\")\n",
        "    return train_fe, test_fe\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 2. ì‹¤í–‰ í”„ë¡œì„¸ìŠ¤\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "# (1) ê¸°ì¡´ ë°ì´í„°ì— í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì ìš©\n",
        "# train, test ë³€ìˆ˜ëŠ” ì‚¬ìš©ìê°€ ì´ë¯¸ ë¡œë“œí•´ ë‘” ì›ë³¸ ë°ì´í„°ë¼ê³  ê°€ì •\n",
        "train_new, test_new = engineer_features(train, test)\n",
        "\n",
        "# (2) íƒ€ê²Ÿ ë¶„ë¦¬ ë° Train/Val ì¬ë¶„í• \n",
        "X_new = train_new.drop('Class', axis=1)\n",
        "y_new = train_new['Class']\n",
        "\n",
        "# Stratified Split\n",
        "X_train_new, X_val_new, y_train_new, y_val_new = train_test_split(\n",
        "    X_new, y_new, test_size=0.2, random_state=42, stratify=y_new\n",
        ")\n",
        "\n",
        "# ë¶ˆê· í˜• ë¹„ìœ¨ ì¬ê³„ì‚°\n",
        "ratio_new = y_train_new.value_counts()[0] / y_train_new.value_counts()[1]\n",
        "\n",
        "# (3) LightGBM ëª¨ë¸ ì¬í•™ìŠµ\n",
        "print(f\"\\n[LightGBM ì¬í•™ìŠµ ì‹œì‘ (Scale Pos Weight: {ratio_new:.2f})]\")\n",
        "lgbm_fe = lgb.LGBMClassifier(\n",
        "    objective='binary',\n",
        "    metric='average_precision',\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.02,\n",
        "    num_leaves=31,\n",
        "    max_depth=-1,\n",
        "    scale_pos_weight=ratio_new,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "lgbm_fe.fit(\n",
        "    X_train_new, y_train_new,\n",
        "    eval_set=[(X_val_new, y_val_new)],\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=True)]\n",
        ")\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 3. ê²°ê³¼ ì‹œê°í™” ë° ì¤‘ìš”ë„ í™•ì¸ í•¨ìˆ˜ (ì¬ì‚¬ìš©)\n",
        "# --------------------------------------------------------------------------------\n",
        "def visualize_lgbm_importances(model, X_val, y_val, scoring_metric='average_precision'):\n",
        "    feature_names = X_val.columns.tolist()\n",
        "    booster = model.booster_\n",
        "\n",
        "    # Gain & Split\n",
        "    imp_gain = booster.feature_importance(importance_type='gain')\n",
        "    df_gain = pd.DataFrame({'feature': feature_names, 'importance': imp_gain}).sort_values('importance', ascending=False)\n",
        "\n",
        "    imp_split = booster.feature_importance(importance_type='split')\n",
        "    df_split = pd.DataFrame({'feature': feature_names, 'importance': imp_split}).sort_values('importance', ascending=False)\n",
        "\n",
        "    # Permutation Importance\n",
        "    print(f\"\\n[Permutation Importance ê³„ì‚° ì¤‘... ({scoring_metric})]\")\n",
        "    perm_result = permutation_importance(\n",
        "        model, X_val, y_val, n_repeats=10, scoring=scoring_metric, n_jobs=-1, random_state=42\n",
        "    )\n",
        "    perm_sorted_idx = perm_result.importances_mean.argsort()\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(24, len(feature_names) * 0.45))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    sns.barplot(x='importance', y='feature', data=df_gain, palette='viridis')\n",
        "    plt.title('Feature Importance (Gain)', fontsize=14, fontweight='bold')\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    sns.barplot(x='importance', y='feature', data=df_split, palette='viridis')\n",
        "    plt.title('Feature Importance (Split)', fontsize=14, fontweight='bold')\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.boxplot(perm_result.importances[perm_sorted_idx].T, vert=False, labels=np.array(feature_names)[perm_sorted_idx])\n",
        "    plt.title(f'Permutation Importance ({scoring_metric})', fontsize=14, fontweight='bold')\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# (4) ì‹œê°í™” ì‹¤í–‰\n",
        "visualize_lgbm_importances(lgbm_fe, X_val_new, y_val_new)\n",
        "\n",
        "# (5) ì„±ëŠ¥ ë¹„êµ ì¶œë ¥ (F1-Score)\n",
        "val_pred_proba = lgbm_fe.predict_proba(X_val_new)[:, 1]\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_val_new, val_pred_proba)\n",
        "f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
        "f1_scores = np.nan_to_num(f1_scores)\n",
        "best_f1_new = np.max(f1_scores)\n",
        "\n",
        "print(f\"\\nNew Best Validation F1-Score: {best_f1_new:.6f}\")\n"
      ],
      "metadata": {
        "id": "dAwEbGUcLNk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
        "from scipy.optimize import minimize\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 1. ì„¤ì • ë° ì´ˆê¸°í™”\n",
        "# --------------------------------------------------------------------------------\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# OOF ì €ì¥ìš© í–‰ë ¬ (Sample ìˆ˜ x ëª¨ë¸ ìˆ˜)\n",
        "oof_preds_lgbm = np.zeros(len(X))\n",
        "oof_preds_xgb = np.zeros(len(X))\n",
        "oof_preds_cat = np.zeros(len(X))\n",
        "\n",
        "# Test ì˜ˆì¸¡ ì €ì¥ìš© í–‰ë ¬\n",
        "test_preds_lgbm = np.zeros(len(test_new))\n",
        "test_preds_xgb = np.zeros(len(test_new))\n",
        "test_preds_cat = np.zeros(len(test_new))\n",
        "\n",
        "print(f\"âœ… 3-Model Stratified K-Fold ({n_splits}) with SMOTE ì‹œì‘...\\n\")\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 2. K-Fold í•™ìŠµ ë£¨í”„\n",
        "# --------------------------------------------------------------------------------\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
        "    # (1) ë°ì´í„° ë¶„í• \n",
        "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "    # (2) SMOTE ì ìš© (Train Fold Only!)\n",
        "    # sampling_strategy=0.1 : ì†Œìˆ˜ í´ë˜ìŠ¤ ë¹„ìœ¨ì„ ë‹¤ìˆ˜ì˜ 10%ê¹Œì§€ ì¦ê°•\n",
        "    smote = SMOTE(random_state=42, sampling_strategy=0.1)\n",
        "    X_train_res, y_train_res = smote.fit_resample(X_train_fold, y_train_fold)\n",
        "\n",
        "    print(f\"=== [Fold {fold}/{n_splits}] SMOTE Train Size: {len(y_train_res)} ===\")\n",
        "\n",
        "    # (3) LightGBM í•™ìŠµ\n",
        "    lgb_clf = lgb.LGBMClassifier(\n",
        "        n_estimators=2000, learning_rate=0.02, num_leaves=31, max_depth=-1,\n",
        "        random_state=42, n_jobs=-1, verbose=-1, metric='average_precision'\n",
        "        # SMOTE ì¼ìœ¼ë¯€ë¡œ scale_pos_weight ì œê±°\n",
        "    )\n",
        "    lgb_clf.fit(\n",
        "        X_train_res, y_train_res,\n",
        "        eval_set=[(X_val_fold, y_val_fold)],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]\n",
        "    )\n",
        "\n",
        "    # (4) XGBoost í•™ìŠµ\n",
        "    xgb_clf = xgb.XGBClassifier(\n",
        "        n_estimators=2000, learning_rate=0.02, max_depth=6,\n",
        "        random_state=42, n_jobs=-1, tree_method='hist',\n",
        "        objective='binary:logistic', eval_metric='aucpr',\n",
        "        early_stopping_rounds=100\n",
        "    )\n",
        "    xgb_clf.fit(\n",
        "        X_train_res, y_train_res,\n",
        "        eval_set=[(X_val_fold, y_val_fold)],\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # (5) CatBoost í•™ìŠµ\n",
        "    cat_clf = CatBoostClassifier(\n",
        "        n_estimators=2000, learning_rate=0.02, depth=6,\n",
        "        random_state=42, verbose=0, loss_function='Logloss', eval_metric='PRAUC',\n",
        "        allow_writing_files=False, early_stopping_rounds=100\n",
        "    )\n",
        "    cat_clf.fit(\n",
        "        X_train_res, y_train_res,\n",
        "        eval_set=[(X_val_fold, y_val_fold)]\n",
        "    )\n",
        "\n",
        "    # (6) ì˜ˆì¸¡ ë° ì €ì¥\n",
        "    # OOF Prediction (ê²€ì¦ ë°ì´í„°)\n",
        "    oof_preds_lgbm[val_idx] = lgb_clf.predict_proba(X_val_fold)[:, 1]\n",
        "    oof_preds_xgb[val_idx] = xgb_clf.predict_proba(X_val_fold)[:, 1]\n",
        "    oof_preds_cat[val_idx] = cat_clf.predict_proba(X_val_fold)[:, 1]\n",
        "\n",
        "    # Test Prediction (í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ê°’ ëˆ„ì )\n",
        "    test_preds_lgbm += lgb_clf.predict_proba(test_new)[:, 1] / n_splits\n",
        "    test_preds_xgb += xgb_clf.predict_proba(test_new)[:, 1] / n_splits\n",
        "    test_preds_cat += cat_clf.predict_proba(test_new)[:, 1] / n_splits\n",
        "\n",
        "print(\"\\nâœ… ëª¨ë“  Fold í•™ìŠµ ì™„ë£Œ!\")\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 3. ìµœì  ë¸”ë Œë”© ê°€ì¤‘ì¹˜ ì°¾ê¸° (OOF Ensemble)\n",
        "# --------------------------------------------------------------------------------\n",
        "# ì „ì²´ OOF ì˜ˆì¸¡ê°’ ê²°í•©\n",
        "oof_predictions = np.column_stack((oof_preds_lgbm, oof_preds_xgb, oof_preds_cat))\n",
        "\n",
        "def loss_func(weights):\n",
        "    # ê°€ì¤‘ì¹˜ ì •ê·œí™”\n",
        "    weights /= np.sum(weights)\n",
        "    # ê°€ì¤‘ í‰ê· \n",
        "    final_pred = np.dot(oof_predictions, weights)\n",
        "    # AUPRC ì ìˆ˜ ê³„ì‚° (ìµœëŒ€í™” -> ìµœì†Œí™”)\n",
        "    score = average_precision_score(y, final_pred)\n",
        "    return -score\n",
        "\n",
        "# ìµœì í™” ìˆ˜í–‰\n",
        "init_weights = [1/3, 1/3, 1/3]\n",
        "constraints = ({'type': 'eq', 'fun': lambda w: 1 - np.sum(w)})\n",
        "bounds = [(0, 1)] * 3\n",
        "\n",
        "res = minimize(loss_func, init_weights, method='SLSQP', bounds=bounds, constraints=constraints)\n",
        "best_weights = res.x\n",
        "best_score = -res.fun\n",
        "\n",
        "print(f\"\\n[Blending Optimization Result]\")\n",
        "print(f\"Best OOF AUPRC: {best_score:.6f}\")\n",
        "print(f\"Weights -> LGBM: {best_weights[0]:.4f}, XGB: {best_weights[1]:.4f}, CAT: {best_weights[2]:.4f}\")\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 4. ìµœì¢… Test ì˜ˆì¸¡ ìƒì„± ë° íŒŒì¼ ì €ì¥\n",
        "# --------------------------------------------------------------------------------\n",
        "# Test ì˜ˆì¸¡ê°’ ê°€ì¤‘ í‰ê· \n",
        "final_test_pred = (test_preds_lgbm * best_weights[0] +\n",
        "                   test_preds_xgb * best_weights[1] +\n",
        "                   test_preds_cat * best_weights[2])\n",
        "\n",
        "# ìµœì  ì„ê³„ê°’ ì°¾ê¸° (ì „ì²´ OOF ê¸°ì¤€)\n",
        "oof_blended = np.dot(oof_predictions, best_weights)\n",
        "precision, recall, thresholds = precision_recall_curve(y, oof_blended)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
        "best_thresh = thresholds[np.argmax(np.nan_to_num(f1_scores))]\n",
        "\n",
        "print(f\"Best Threshold (Optimized F1): {best_thresh:.6f}\")\n",
        "\n",
        "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
        "submission['Class'] = (final_test_pred >= best_thresh).astype(int)\n",
        "submission.to_csv('submission_kfold_smote_blended.csv', index=False)\n",
        "print(\"submission_kfold_smote_blended.csv ì €ì¥ ì™„ë£Œ\")\n"
      ],
      "metadata": {
        "id": "SsgJCaTXLNif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
        "from scipy.optimize import minimize\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import optuna\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 0. Optuna ìµœì í™” ì„¤ì • (ì‹œê°„ ì œì•½ ê³ ë ¤)\n",
        "# --------------------------------------------------------------------------------\n",
        "# ì „ì²´ 15ë¶„ ì¤‘, ëª¨ë¸ë³„ ìµœì í™”ì— ì•½ 5~6ë¶„ì”© í• ë‹¹ (ì´ 10~12ë¶„ ì†Œìš” ì˜ˆì •)\n",
        "OPTUNA_TIMEOUT = 300  # ëª¨ë¸ë‹¹ 300ì´ˆ (5ë¶„) ì œí•œ\n",
        "\n",
        "# Optunaìš© ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (ì†ë„ë¥¼ ìœ„í•´ Hold-out ë°©ì‹ ì‚¬ìš©)\n",
        "X_opt_train, X_opt_val, y_opt_train, y_opt_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Optuna ë‚´ë¶€ì—ì„œë„ SMOTE ì ìš© (ë°ì´í„° ë¶„í¬ ë§ì¶”ê¸° ìœ„í•¨)\n",
        "print(\"â³ Optunaìš© SMOTE ì ìš© ì¤‘...\")\n",
        "smote_opt = SMOTE(random_state=42, sampling_strategy=0.1)\n",
        "X_opt_train_res, y_opt_train_res = smote_opt.fit_resample(X_opt_train, y_opt_train)\n",
        "print(\"âœ… Optuna ì¤€ë¹„ ì™„ë£Œ\\n\")\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 1. LightGBM Optuna Objective\n",
        "# --------------------------------------------------------------------------------\n",
        "def objective_lgbm(trial):\n",
        "    params = {\n",
        "        'n_estimators': 2000,\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'verbose': -1,\n",
        "        'metric': 'average_precision'\n",
        "    }\n",
        "\n",
        "    model = lgb.LGBMClassifier(**params)\n",
        "\n",
        "    # Pruningì„ ìœ„í•œ Callback ì„¤ì •\n",
        "    callbacks = [\n",
        "        lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
        "        optuna.integration.LightGBMPruningCallback(trial, \"average_precision\")\n",
        "    ]\n",
        "\n",
        "    model.fit(\n",
        "        X_opt_train_res, y_opt_train_res,\n",
        "        eval_set=[(X_opt_val, y_opt_val)],\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Best score ë°˜í™˜\n",
        "    return model.best_score_['valid_0']['average_precision']\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 2. XGBoost Optuna Objective\n",
        "# --------------------------------------------------------------------------------\n",
        "def objective_xgb(trial):\n",
        "    params = {\n",
        "        'n_estimators': 2000,\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'tree_method': 'hist',\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'aucpr',\n",
        "        'early_stopping_rounds': 50\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBClassifier(**params)\n",
        "\n",
        "    # Pruningì€ ë³µì¡í•˜ë¯€ë¡œ ë‹¨ìˆœ Early Stoppingë§Œ ì ìš©í•˜ê±°ë‚˜ ì§ì ‘ êµ¬í˜„í•´ì•¼ í•¨\n",
        "    # ì—¬ê¸°ì„œëŠ” ì‹œê°„ ê´€ê³„ìƒ fit ë‚´ë¶€ ê¸°ëŠ¥ë§Œ í™œìš©\n",
        "    model.fit(\n",
        "        X_opt_train_res, y_opt_train_res,\n",
        "        eval_set=[(X_opt_val, y_opt_val)],\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    preds = model.predict_proba(X_opt_val)[:, 1]\n",
        "    score = average_precision_score(y_opt_val, preds)\n",
        "    return score\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 3. ìµœì í™” ì‹¤í–‰ (Time Limit ì ìš©)\n",
        "# --------------------------------------------------------------------------------\n",
        "print(f\"ğŸš€ LightGBM ìµœì í™” ì‹œì‘ (ìµœëŒ€ {OPTUNA_TIMEOUT}ì´ˆ)...\")\n",
        "study_lgbm = optuna.create_study(direction='maximize')\n",
        "study_lgbm.optimize(objective_lgbm, timeout=OPTUNA_TIMEOUT) # ì‹œê°„ ì œí•œ\n",
        "print(f\"LGBM Best Params: {study_lgbm.best_params}\")\n",
        "\n",
        "print(f\"\\nğŸš€ XGBoost ìµœì í™” ì‹œì‘ (ìµœëŒ€ {OPTUNA_TIMEOUT}ì´ˆ)...\")\n",
        "study_xgb = optuna.create_study(direction='maximize')\n",
        "study_xgb.optimize(objective_xgb, timeout=OPTUNA_TIMEOUT) # ì‹œê°„ ì œí•œ\n",
        "print(f\"XGB Best Params: {study_xgb.best_params}\")\n",
        "\n",
        "# ìµœì  íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸° (ê³ ì • íŒŒë¼ë¯¸í„° ë³‘í•©)\n",
        "best_params_lgbm = study_lgbm.best_params\n",
        "best_params_lgbm.update({\n",
        "    'n_estimators': 2000, 'random_state': 42, 'n_jobs': -1, 'verbose': -1, 'metric': 'average_precision'\n",
        "})\n",
        "\n",
        "best_params_xgb = study_xgb.best_params\n",
        "best_params_xgb.update({\n",
        "    'n_estimators': 2000, 'random_state': 42, 'n_jobs': -1, 'tree_method': 'hist',\n",
        "    'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'early_stopping_rounds': 100\n",
        "})\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 4. ìµœì¢… 5-Fold í•™ìŠµ ë° ì¶”ë¡  (ê¸°ì¡´ ë¡œì§ + ìµœì  íŒŒë¼ë¯¸í„°)\n",
        "# --------------------------------------------------------------------------------\n",
        "print(\"\\nâœ… ìµœì¢… 3-Model Stratified K-Fold í•™ìŠµ ì‹œì‘ (Optimized Params)...\")\n",
        "\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# ì €ì¥ìš© ë°°ì—´ ì´ˆê¸°í™”\n",
        "oof_preds_lgbm = np.zeros(len(X))\n",
        "oof_preds_xgb = np.zeros(len(X))\n",
        "oof_preds_cat = np.zeros(len(X))\n",
        "\n",
        "test_preds_lgbm = np.zeros(len(test_new))\n",
        "test_preds_xgb = np.zeros(len(test_new))\n",
        "test_preds_cat = np.zeros(len(test_new))\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
        "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "    # SMOTE\n",
        "    smote = SMOTE(random_state=42, sampling_strategy=0.1)\n",
        "    X_train_res, y_train_res = smote.fit_resample(X_train_fold, y_train_fold)\n",
        "\n",
        "    if fold == 1:\n",
        "        print(f\"=== Fold 1 SMOTE Size: {len(y_train_res)} ===\")\n",
        "\n",
        "    # (1) Optimized LightGBM\n",
        "    lgb_clf = lgb.LGBMClassifier(**best_params_lgbm)\n",
        "    lgb_clf.fit(\n",
        "        X_train_res, y_train_res,\n",
        "        eval_set=[(X_val_fold, y_val_fold)],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]\n",
        "    )\n",
        "\n",
        "    # (2) Optimized XGBoost\n",
        "    xgb_clf = xgb.XGBClassifier(**best_params_xgb)\n",
        "    xgb_clf.fit(\n",
        "        X_train_res, y_train_res,\n",
        "        eval_set=[(X_val_fold, y_val_fold)],\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # (3) CatBoost (ê¸°ì¡´ ì„¤ì • ìœ ì§€)\n",
        "    cat_clf = CatBoostClassifier(\n",
        "        n_estimators=2000, learning_rate=0.02, depth=6,\n",
        "        random_state=42, verbose=0, loss_function='Logloss', eval_metric='PRAUC',\n",
        "        allow_writing_files=False, early_stopping_rounds=100\n",
        "    )\n",
        "    cat_clf.fit(\n",
        "        X_train_res, y_train_res,\n",
        "        eval_set=[(X_val_fold, y_val_fold)]\n",
        "    )\n",
        "\n",
        "    # OOF & Test Predict\n",
        "    oof_preds_lgbm[val_idx] = lgb_clf.predict_proba(X_val_fold)[:, 1]\n",
        "    oof_preds_xgb[val_idx] = xgb_clf.predict_proba(X_val_fold)[:, 1]\n",
        "    oof_preds_cat[val_idx] = cat_clf.predict_proba(X_val_fold)[:, 1]\n",
        "\n",
        "    test_preds_lgbm += lgb_clf.predict_proba(test_new)[:, 1] / n_splits\n",
        "    test_preds_xgb += xgb_clf.predict_proba(test_new)[:, 1] / n_splits\n",
        "    test_preds_cat += cat_clf.predict_proba(test_new)[:, 1] / n_splits\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 5. ë¸”ë Œë”© ë° ê²°ê³¼ ì €ì¥ (ê¸°ì¡´ ë¡œì§ ë™ì¼)\n",
        "# --------------------------------------------------------------------------------\n",
        "oof_predictions = np.column_stack((oof_preds_lgbm, oof_preds_xgb, oof_preds_cat))\n",
        "\n",
        "def loss_func(weights):\n",
        "    weights /= np.sum(weights)\n",
        "    final_pred = np.dot(oof_predictions, weights)\n",
        "    score = average_precision_score(y, final_pred)\n",
        "    return -score\n",
        "\n",
        "init_weights = [1/3, 1/3, 1/3]\n",
        "constraints = ({'type': 'eq', 'fun': lambda w: 1 - np.sum(w)})\n",
        "bounds = [(0, 1)] * 3\n",
        "\n",
        "res = minimize(loss_func, init_weights, method='SLSQP', bounds=bounds, constraints=constraints)\n",
        "best_weights = res.x\n",
        "best_score = -res.fun\n",
        "\n",
        "print(f\"\\n[Final Blending Result]\")\n",
        "print(f\"Best AUPRC: {best_score:.6f}\")\n",
        "print(f\"Weights -> LGBM: {best_weights[0]:.4f}, XGB: {best_weights[1]:.4f}, CAT: {best_weights[2]:.4f}\")\n",
        "\n",
        "final_test_pred = (test_preds_lgbm * best_weights[0] +\n",
        "                   test_preds_xgb * best_weights[1] +\n",
        "                   test_preds_cat * best_weights[2])\n",
        "\n",
        "oof_blended = np.dot(oof_predictions, best_weights)\n",
        "precision, recall, thresholds = precision_recall_curve(y, oof_blended)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
        "best_thresh = thresholds[np.argmax(np.nan_to_num(f1_scores))]\n",
        "\n",
        "print(f\"Best Threshold: {best_thresh:.6f}\")\n",
        "\n",
        "submission['Class'] = (final_test_pred >= best_thresh).astype(int)\n",
        "submission.to_csv('submission_optuna_blended.csv', index=False)\n",
        "print(\"submission_optuna_blended.csv ì €ì¥ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "I3aAom65LNYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p-G3KFyaLNVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l3t4p4pYLNTh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}